<!DOCTYPE html>
<html>

<head>
	<title>An Interactive Introduction to Model-Agnostic Meta-Learning üë©‚Äçüî¨</title>
	<meta charset="UTF-8" />
	<link rel="stylesheet" href="build/bundle.css">
</head>

<body>
	<script async src="https://distill.pub/template.v1.js"></script>
	<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

	<script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>
	<script defer src="build/bundle.js"></script>

	<script type="text/front-matter">
  title: "An Interactive Introduction to Model-Agnostic Meta-Learning"
  description: "t.b.d."
  authors:
  - Luis M√ºller: https://github.com/pupuis
  - Max Ploner:
  affiliations:
  - NI @ TU Berlin: https://www.ni.tu-berlin.de/menue/neural_information_processing_group/
  - NI @ TU Berlin: https://www.ni.tu-berlin.de/menue/neural_information_processing_group/
</script>

	<dt-article>
		<h1>An Interactive Introduction to Model-Agnostic Meta-Learning</h1>
		<h2>Exploring the world of model-agnostic meta-learning and its extensions.</h2>
		<figure class="l-page" style="border-top: 1px solid hsla(0, 0%, 0%, 0.1);">
			<d-figure id="teaser">
				<div class="element is-loading" style="height: 400px"></div>
			</d-figure>
			<figcaption>
				Click button or drag sample onto example to classify the sample.
			</figcaption>
		</figure>

		<dt-byline></dt-byline>

		<p>
			<i style="font-size: .8em;">
				If you are already familiar with few-shot learning, and are only interested
				in learning about MAML specifically, you may want to skip this section and
				jump straight to the <a href="#few_shot_maml">next part</a>.
			</i>
		</p>

		<p>
			If you tried the little exercise above, you probably got a pretty good score.
			Even though you likely have never seen some of the characters,
			you are able to classify them given only a single example, potentially
			without realizing that what you are doing is actually pretty impressive!
		</p>

		<h3>Few-Shot Learning</h3>

		<p>
			Typically, in machine learning we need a lot of examples to be able to
			assign an unseen sample to class. But we do not always have enough data to
			cater this need: A sufficient amount of data may be expensive or even
			impossible to acquire.
			But there is good reasons to believe, that this is not an inherent issue
			of learning.
			Humans, for example, are quite good at generalizing after seeing only a few
			samples. Enabling machine learning methods to achieve the same would allow
			for many new applications and generally reduce the need to collect huge
			datasets.[<b>TODO: reference needed</b>]
		</p>

		<p>
			This research area is called "few-shot learning". It aims to enable models
			to classify unseen sampels after training only one a few examples ("shots").
			The little exercise at the top of the article is an exmaple of such a task.
			The symbols are part of a
			<dt-cite key="lake_omniglot_2015">dataset called "omniglot"</dt-cite>.
			It contains 1623 different characters across 50 alphabets and each character
			has 20 instances drawn by different people. Because of that, the omniglot
			was described as a "transpose" of the well-known MNIST dataset, by the authors
			who originally proposed omniglot as a machine learning dataset.
			<dt-cite key="lake_one_2011"></dt-cite>
			While MNIST contains only a few classes (the digits 0 to 9) and many instances,
			of each class, with omniglot it is vice-versa.
		</p>



		<p>
			By now, you may be asking yourself: How can a model learn from only one example
			(i.e. "one-shot") and why do I even need a dataset like omniglot
			if the model is supposed to learn from one sample?

			Looking at a human learner, we might realize that humans do not learn
			novel concepts "in a vacuum".<dt-cite key="lake_one_2011"></dt-cite>
			Instead, past experience supports the ability to learn the novel concept.
		</p>

		<p>
			While clearly one sample is not enough for a model without prior knowledge,
			we can pretrain models on tasks that we assume to be similar to the target tasks.
			This is a common approach to solving few-shot learning: derive some inductive bias
			from other classes in order to perform better on newly encountered classes.
			This similarity assumption allows the model to collect meta-knowledge:
			knowledge which does not describe a single task, but a distribution of tasks.
			The learning of this meta-knowledge is referred to as "meta-learning".
		</p>

		<figure class="l-page side">
			<d-figure id="fewShotMethods">
				<div class="element is-loading" style="height: 400px"></div>
			</d-figure>
			<figcaption>
				<p>Results of different methods on the omniglot dataset
					(if not stated differently: 1-shot, 20-way;
					some differences in the evaluation procedure exist).
					As usual, accuracy numbers need to be taken with a grain of salt as
					differences in the evaluation method, implementation, and the model
					complexity may a have a non-negligible impact on the performance.
				</p>
				<p id="caption_generative_stroke_model" class="fewShotMethods-reference">
					The generative stroke model was introduced in the paper which also
					introduced omniglot. The model is based on a latent stroke representation
					(including the number and directions of strokes). While it is an
					interesting approach, it can hardly be generalized to other
					few-shot problems.<dt-cite key="lake_one_2011"></dt-cite>
				</p>
				<p id="caption_hierachical_bayesian_program_learning" class="fewShotMethods-reference">
					The same authors improved the model by learning latent
					primitive motor elements and called this process "Hierarchial
					Bayesian Program Learning" (HBPL).
					While the accuracy was greatly increased, it also
					is focused on symbol learning.
					<dt-cite key="lake_one-shot_2013"></dt-cite>
				</p>
				<p id="caption_siamese_nets" class="fewShotMethods-reference">
					Siames Nets consist of two identical networks which produce a latent representation.
					From the representations of two samples, a distance is calculated to
					assess the similarity of the two samples.<dt-cite key="koch_siamese_2015"></dt-cite>
					The result (accuracy of 88.1%) results from a reimplementaion of the method which
					makes it more comparable.
					<dt-cite key="vinyals_matching_2017"></dt-cite>
				</p>
				<p id="caption_matching_nets" class="fewShotMethods-reference">
					Matching Networks also work by comparing new samples to labeled
					examples. They do so by utilizing an attention kernel.
					Though the second version of the paper is cited here,
					it was first published in 2016.
					<dt-cite key="vinyals_matching_2017"></dt-cite>
				</p>

				<p id="caption_prototypical_networks" class="fewShotMethods-reference">
					Prototypical Networks use prototype vectors to represent each class
					in the metric space. The nearest neighbor (i.e. the closest prototype)
					of a sample then determines the prediction.
					<dt-cite key="snell_prototypical_2017"></dt-cite>
				</p>

				<p id="caption_memory_augmented_nets" class="fewShotMethods-reference">
					Memory-Augmented Networks (MANNs) use an external memory to make accurate
					predictions using a small number of samples.
					<dt-cite key="santoro_meta-learning_2016"></dt-cite>
				</p>

				<p id="caption_meta_nets" class="fewShotMethods-reference">
					Meta Networks utilize a base learner (task level) and a meta learner
					as well as a set of slow and rapid weights in order to allow
					meta learning as well as task-specific concepts.
					<dt-cite key="munkhdalai_meta_2017"></dt-cite>
				</p>

			</figcaption>
		</figure>

		<figure class="l-page side">
			<d-figure id="fewShotVenn">
				<div class="element is-loading" style="height: 300px"></div>
			</d-figure>
			<figcaption>
				<p id="caption_lstm_based" class="fewShotMethods-reference">
					Applications of Meta-Learning outside the domain of few-shot learning
					include the optimization of the task-level optimizer using
					a LSTM network.
					<dt-cite key="andrychowicz_learning_2016"></dt-cite>
				</p>
			</figcaption>
		</figure>



		<p>
			Plenty of methods have been proposed in the last years to tackle few-shot
			learning using the meta-learning assumption,
			and many are able to produce decent results.
			In this article, we will focus
			on one of them: "Model-Agnostic Meta-Learning" (MAML).
		</p>

		<h3>Model-Agnostic Meta-Learning</h3>

		<p>
			Before we get started with the explanation of how MAML works,
			we want to take a moment to look at what is Model-Agnostic about MAML.
			In order to understand, why this method is called "model-agnostic", we need
			to look at how the few-shot problem was tackled by other approaches.
			While there are methods, which try to mitigate the issues introduced by the
			the few-shot constraint, many do resort to adopting the meta-learning
			assumption. These methods can broadly be divided into two classes:
			metric-based and model-based approaches.
		</p>
		<p>
			The core idea of metric-based approaches is two compare two samples in a
			latent (metric) space: In this space, samples of the same class are supposed
			to be close to each other, while two samplese from different classes are
			supposed to have a large distance (the notion of a distance is what makes
			the latent space a metric space).
			<dt-cite key="snell_prototypical_2017"></dt-cite>
			<dt-cite key="koch_siamese_2015"></dt-cite>
			<dt-cite key="vinyals_matching_2017"></dt-cite>
		</p>
		<p>
			Model-based approaches are neural architectures which are deliberately designed
			for fast adaption to new tasks without inclining to overfit.
			Memory-Augmented Neural Networks and MetaNet are two examples. Both employ
			an external memory while still maintaining the ability to be trained
			end-to-end.
			<dt-cite key="santoro_meta-learning_2016"></dt-cite>
			<dt-cite key="munkhdalai_meta_2017"></dt-cite>
		</p>

		<p>
			MAML goes a different route: The nerual network is designed the same way
			your usual model might be (in the many-shot case). All the magic happens during the
			optimization. Hence, it is called "optimization-based".
			Unlike, metric-based and model-based approaches, MAML lets
			you choose the model architecture as you like.
			This has the great benefit of being applicable can not only for
			classical supervised learning classification tasks but can also
			for reinforcement learning.
			<dt-cite key="finn2017modelagnostic"></dt-cite>
		</p>
		<p>
			In the next seciton, you will learn how this is achieved.
		</p>

		<h2 id="few_shot_maml">Few-shot learning with MAML</h2>
		<p>Model-Agnostic Meta Learning (MAML) is a meta-learning approach to solve few-shot learning.
			To learn more about it, let us build an example from the ground up and then try to apply MAML.
			We will do this by alternating mathematical walk-throughs and interactive, as well as, coding examples.
		</p>
		<!--TODO: Maybe card to github repo for all code: https://github.com/lepture/github-cards-->
		<p>
			If you have done machine learning
			before you have probably already solved or attempted to solve a problem like the following:
			Learning a model to solve one specific task, for example to classify cats from dogs or to
			teach an agent
			to find its way through a specific maze. In those settings, if we are able to define a loss
			\(\mathcal{L}_\tau\) for our task \(\tau\) which depends on the parameters
			\(\phi\) of a model, we can express our learning objective as

			\[ \underset{\phi}{\text{min}} \, \mathcal{L}_\tau (\phi) .\]

			We usually find the optimal \(\phi\) by progressively walking along the direction of the gradient of
			\(\mathcal{L}_\tau\) with respect to \(\phi\), i.e.

			\[ \phi \leftarrow \phi - \alpha \nabla_\phi \mathcal{L}_\tau (\phi) ,\]

			also known as gradient descent, where \(\mathcal{L}_\tau\) usually also depends on some data and \(\alpha\)
			is a fixed learning rate,
			controlling the size of the steps we want to take.
		</p>
		<p>Unfortunately, when framing this in a few-shot setting (i.e., with a very small dataset), the above method is
			known to fail almost certaintly [REF]. A key insight to MAML is now to
			bypass this problem by learning not only from the data regarding exactly our task, but to learn also from
			data of similar tasks.
			To incorporate this we make an additional assumption, namely that \(\tau\) comes from some distribution of
			tasks \(p(\tau)\) and that we
			can sample freely from this distribution. We can then generalize the above objective to learn how to find an
			optimization strategy for a randomly sampled task from \(p(\tau)\), which we can express as follows:

			\[ \text{min} \, \mathbb{E}_\tau [ \mathcal{L}_\tau (\phi_\tau) ] ,\]

			where \(\tau\) is now a random variable and \(\phi_\tau\) are a set of parameters for task \(\tau\).
			We may use different parameters for each task, use the same parameters for every task or something in
			between. So we have to decide on a parameterization and decide how we actually minimize this new objective.
			In the following we will explore two possible
			answers to these issues.
		</p>
		<h3>Part 1: Just do gradient descent, silly!</h3>
		<p>Of course! The answer to most problems. We can simply learn a single set of parameters \(\phi\) which
			minimizes the expected loss
			over all tasks. Or formally speaking our objective becomes

			\[ \underset{\phi}{\text{min}} \, \mathbb{E}_\tau [ \mathcal{L}_\tau (\phi) ] .\]

			As you can see we chose the most simple parameterization: We learn one parameter vector \(\phi\) that
			minimizes the objective
			across all tasks.
			Further, to actually implement this strategy we can do what is known as <i>Expected Risk Minimization
				(ERM)</i>, which tells us to sample a lot of tasks \(\tau\) and then descent according to

			\[ \phi \leftarrow \phi - \alpha \nabla_\phi \sum_i \mathcal{L}_{\tau_i} (\phi) .\]

			Finn et al. (the authors of MAML) call this type of model <i>pretrained</i>, referring to it simply
			pretraining over all available data and
			then trying to converge on a small amount of samples later.
		</p>
		<h4>Implementing the Pretrained Model</h4>
		<p>
			If the above has gotten all too theoretic for you, take a look at the following <i>gist</i>. It contains a
			simplistic
			implementation of an update step for this <i>pretrained</i> model. It is implemented such as to emphesize
			that
			even if we differentiate between tasks when sampling the batch, the actual optimizer treats each sample the
			same.
		</p>
		<script src="https://gist.github.com/pupuis/cf31417164bc5513302788b906d2a3c2.js"></script>
		<p>
			The implementation is agnostic to the choice of optimizer that you use. To train their <i>pretrained</i>
			model, the MAML creators use
			the <i>Adam</i> optimizer.
		</p>
		<h4>Pretrained Model on a Sinusoid Problem</h4>
		<p>
			In the following figure you can experiment with a <i>pretrained</i> model, which was trained by a collection
			of
			sinusoid regression tasks. The task distribution works as follows: Each task is represented by an amplitude
			\(A\) and a
			phase \(\varphi\) and requires the prediction of sinusoid \(f\):

			\[ f(x) := A \sin(x + \varphi),\]

			where \(A, \varphi\) are sampled uniformly from some predefined range.

			The jist is that different parameters yield different functions \(f_1\) and \(f_2\) with possibly completely
			different function values and gradients.
			Take for example the following two tasks:
			Tasks \(\tau_1, \tau_2\) are both regression tasks on sinusoids \(f_1(x) := \sin (x - \frac{\pi}{2})\) and
			\(f_2(x) := \sin (x + \frac{\pi}{2})\) respectively. These two tasks' function values give completely
			contradicting information, as

			\[ f_1(x) = - f_2(x). \]
		</p>
		<p>
			Before you fit the model, think what
			you would expect to happen based on the position and the
			amount of samples you provided. Feel free to also experiment with the different settings: Distributing the
			samples equispaced or squeezing all of them to a small range
			of the x-axis.
		</p>
		<figure>
			<d-figure id="fitSinePretrained"></d-figure>
		</figure>
		<p>Ouch! That doesn't seem to work that well. Maybe you have already guessed that this would have been to easy.
			The problem that the above approach faces is that the optimal parameters of
			different tasks might live in completely different loss spaces. Given a loss function \(\mathcal{L}\) w.r.t
			some parameters \(\rho\), as well as
			constant parameter-set \(C\) (e.g. a dataset on which the loss is defined), the parameter space defined the
			function values of

			\[ \mathcal{L}(\rho; C) .\]

			You can investigate the problem of different loss spaces for different tasks a little deeper in
			<a href="#twoLossSpaces">this figure</a>. The loss spaces displayed stem from a curve fitting (toy) problem,
			where
			we are interested in parameters \(a, b\)
			and the task requires the prediction of

			\[g(x) = \textbf{elu}(ax + b),\]

			where \(\textbf{elu}\) is the <i>Exponential Linear Unit</i>, which is usually employed as an activation
			function in neural nets, but serves
			us here to make the resulting loss spaces a bit more funky. For reference, this is how the \(\textbf{elu}\)
			is
			defined:

			\[ \textbf{elu} = \begin{cases}
			x & \, x >= 0 \\
			\alpha (\exp(x) - 1) & \, \text{otherwise}.
			\end{cases}
			\]
		</p>
		<figure>
			<d-figure id="twoLossSpaces"></d-figure>
		</figure>
		<p>
			At this point we can conclude that optimizating for parameter-vector \(\phi\) minimizing all task
			distributions at the same time
			is maybe not the best parameter to optimize for. The next approach rather optimizes another variable of the
			problem. Maybe you already have a hunch?
		</p>

		<h3>Part 2: Just do Model-Agnostic Meta-Learning, silly!</h3>
		<p>
			MAML comes to our rescue. We have already seen how different tasks might give us contradicting function
			values at the same query point and how that leads to an undesired ping-pong from one local optimum to
			another, resulting in no meaningful convergence. The idea of MAML is now to
			learn an optimal initialization \(\theta\) in the sense that initializating a task optimizer with these
			parameters and taking gradient descent steps
			from there minimizes the loss for that task. We can express this formally as follows:

			\[ \underset{\theta}{\text{min}} \, \mathbb{E}_\tau [ \mathcal{L}_\tau (U_\tau(\theta)) ] .\]

			where \(U_\tau\) is an optimizer that takes a number of gradient descent steps given the samples of task
			\(\tau\). This is also why MAML is called mode-agnostic, it does
			not make any further assumptions about \(U_\tau\), thus allowing any gradient-based inner optimizer.
		</p>
		<h4>Outline of the Algorithm</h4>
		<p>
			Now that we fixed the objective function, let us briefly take a look at the three main steps of the method,
			given a (current)
			meta-parameter \(\theta\):
		</p>
		<ul>
			<li>1. Sample a number of tasks \(\tau_i\) from \(p(\tau)\).</li>
			<li>2. For each task obtain \(\phi_i = U_{\tau_i}(\theta)\), by minimizing \(\mathcal{L}_{\tau_i,
				\text{train}}(\theta)\)
				on a few training samples.</li>
			<li>3. Update \(\theta\) by gradient descent such that it minimizes \(\mathcal{L}(\theta) := \sum_i
				\mathcal{L}_{\tau_i, \text{test}}(\phi_i)\) on a few
				test samples.</li>
		</ul>
		<p>
			Note that \(\mathcal{L}_{\tau_i, \text{train}}\) and \(\mathcal{L}_{\tau_i, \text{test}}\) are two instances
			of the same loss function
			for different tasks and corresponding training or test data from these tasks.
			Obtaining the \(\phi_i\) is easy, we just do gradient descent:

			\[ \phi_i = \theta - \alpha \nabla_\theta \mathcal{L}_{\tau_i, \text{train}}(\theta).\]

			Further, updating \(\theta\) requires us to evaluate the gradient of the individual task losses on a set of
			test data. We obtain
			the gradient of the overall loss as follows:

			\[ \nabla_\theta \mathcal{L}(\theta) = \sum_{i} \nabla_\theta
			\mathcal{L}_{\tau_i, \text{test}}(\phi_i)
			.\]

			Note that \(\phi_i = U_{\tau_i}(\theta)\) depends on \(\theta\), which means that we have to take a gradient
			through the optimizer
			\(U\). We can then update \(\theta\) via gradient descent, using a new learning rate \(\beta\):

			\[ \theta' = \theta - \beta \nabla_\theta \mathcal{L}(\theta).\]

			And that ü•Å... is more or less everything that comprises the original MAML algorithm.
		</p>
		<h4>Implementing the Algorithm</h4>
		<p>
			However, a machine learning algorithm is not very usefull unless we can execute it on a computer. While
			implementing the <i>pretrained</i>
			model was more or less straightforward, implementing MAML requires some more thinking. Firstly, computing
			\(\phi_i\) is still straightforward, simply call
			the optimization algorithm of your choice (as long as it is gradient-based).
			However, how do we then take the gradient through that optimization algorithm?
			It's actually not that complicated. Almost every modern machine learning framework (e.g. tensorflow), can
			differentiate through
			nearly arbitrary python code. Hence, if we can express our optimizer in a python function, then tensorflow
			can differentiate through it.
		</p>
		<p>
			Below you find a <i>gist</i> that implements a simplistic version of the MAML update step. We coded our
			optimizer ourselves within the function
			<i>fastWeights</i>, but the function also directly applies an input tensor to the optimized weights. We did
			this mainly for simplicity but if you
			are interested in a more thorough reasoning about this design choice, you can read more about it <a
				href="https://gist.github.com/pupuis/f23f483c405b0a169bf279f7b02209bc">in the comments under the
				gist</a>.
		</p>
		<script src="https://gist.github.com/pupuis/f23f483c405b0a169bf279f7b02209bc.js"></script>
		<h4>Will it really work?</h4>
		<p>
			Before we study the MAML model on the sinusoid task distribution, let us spend some
			time on trying to understand MAML intuitively. Why would we expect it to produce better results than the
			pretrained model? To answer this, recall the exercise of <a href="#twoLossSpaces">comparing loss spaces</a>
			from before.
			As stated above, MAML tries to learn an optimal initialization \(\theta\), from which we can easily descent
			into all desired task spaces.
			If you go back to the figure you can see that with the right learning rate, it is relatively easy to
			position \(\theta\) such that
			both \(\phi_i\) point to the minimum of their respective tasks. This is exactly what MAML does! And now you
			know first hand, why it might actually work.
		</p>
		<h4>Returning to Sinusoids</h4>
		<p>
			After having studied the math behind the MAML objective, as well as its intuition and implementation, it is
			time to evaluate it on the sinusoid example.
			Hopefully, MAML will produce better results
			than the pretrained model (although by now you should be convinced that it will).
			You will have the opportunity to repeat the above experiements on a model that has been trained with
			MAML in <a href="#fitSineMaml">this figure</a>.
			Try to compare the optimization behavior of both the pretrained model and MAML and evaluate for yourself
			whether
			you think the MAML-trained model has found a good meta-initialization parameter \(\theta\).
		</p>
		<figure>
			<d-figure id="fitSineMaml"></d-figure>
		</figure>
		<p>
			So as you were hopefully able to verify, MAML produces results that are way closer to the actual sinusoid,
			despite being exposed to at most 5 samples.
		</p>
		<p>
			The rest of this article is dedicated to introducing interesting extensions of MAML, making the method
			either more robust or more efficient.
			If you are interested in the former, <a href="#section_imaml">jump to iMAML</a>. If you are more interested
			in the latter, the following section sheds
			a light on the computational (in-)efficiency of MAML and what possible solutions to that problem are.
		</p>
		<h2>First-Order Methods: Why differentiating through an optimizer is actually as complicated as it sounds</h2>
		<p>
			In this section we want to gain some understanding of what it actually means to differentiate through \(U\),
			the optimizer.
			Recall the gradient that MAML requires us to compute:

			\[ \nabla_\theta \mathcal{L}(\theta) = \sum_{i} \nabla_\theta
			\mathcal{L}_{\tau_i, \text{test}}(\phi_i)
			.\]

			Expanding
			the summands by applying the chain rule yields

			\[ \nabla_\theta
			\mathcal{L}_{\tau_i, \text{test}}(\phi_i) = \nabla_\theta
			\mathcal{L}_{\tau_i, \text{test}}(U_{\tau_i}(\theta)) = \nabla_{U_{\tau_i}(\theta)} \mathcal{L}_{\tau_i,
			\text{test}}
			\nabla_\theta U_{\tau_i}(\theta) .\]

			Here, \( \nabla_{U_{\tau_i}(\theta)} \mathcal{L}_{\tau_i, \text{test}} \) represents the gradient of the
			loss of task
			\(\tau_i\) by the optimized parameter \(\phi_i\)
			and \(\nabla_\theta U_{\tau_i}(\theta)\) is a gradient through an optimization algorithm. Even if we assume
			that
			the optimizer takes only one gradient descent step, this term becomes

			\[ \nabla_\theta U_{\tau_i}(\theta) = \nabla_\theta (\theta - \alpha \nabla_\theta
			\mathcal{L}_{\tau_i, \text{train}}(\theta) )\]

			which is equal to

			\[ I - \alpha \nabla^2_\theta \mathcal{L}_{\tau_i, \text{train}}(\theta). \]

			Hence, MAML requires us to compute second derivatives in order to optimize \(\theta\), which is
			computationally inefficient, especially
			in high-dimensional problems (such as learning neural nets).
		</p>
		<p>
			We will now spend some time on introducing the two most prominent solutions to this problem, as well as
			comparing them to MAML and to each other.
		</p>

		<h3>First-Order MAML (FOMAML)</h3>
		<p>
			FOMAML was suggested by Finn et al. (the authors of MAML) and is a straightforward heuristic to get rid of
			the second order terms:
			Setting them to zero! As a result

			\[\nabla_\theta U_{\tau_i}(\theta) = I\]

			and the overall meta loss gradient reduces to

			\[ \nabla_\theta \mathcal{L}(\theta) = \sum_{i} \nabla_{U_{\tau_i}(\theta)} \mathcal{L}_{\tau_i,
			\text{test}}
			.\]

			That's already it. We will be able to see how that change effects the update direction of \(\theta\). But
			first we study another prominent fist-order method, with a slightly different approach.
		</p>

		<h3>Reptile</h3>
		<p>
			Reptile, proposed by Nichol et al., [REF] is another first-order version of MAML that uses a simple update
			procedure to find the
			optimal initialization \(\theta\). Let's have a look:
		</p>
		<ul>
			<li>1. Sample task \(\tau_i\) from \(p(\tau)\)</li>
			<li>2. Compute \(\phi_i := U_{\tau_i}(\theta)\), where \(U_{\tau_i}(\theta)\) is a gradient-based optimizer
				with \(k > 1\) gradient
				descent steps.
			</li>
			<li>3. Update \(\theta\) according to \(\theta = \theta + \beta(\phi_i - \theta)\).</li>
		</ul>
		<p>
		Note the two most obvious differences to MAML: First of all, we sample one task at a time, which is more similar
		how we trained the <i>pretrained</i>
		model than to MAML. Remember that in MAML we minimize the expected meta-loss over a number of sampled tasks.
		Secondly, it seems like we
		aren't computing a meta-gradient at all, more like a difference of parameters. And in fact, the formula we use for
		updating \(\theta\), i.e.

		\[ \theta = \theta + \beta(\phi_i - \theta), \]

		is the formula for linearly interpolating between \( \theta \) and \( \phi_i \), with interpolation rate \(\beta
		\in [0, 1]\).
		If you are not sure what that means, take a look at <a href="#reptileInterpolation">this figure</a>. There you
		can play around with the
		position of \(\theta\), the optimizer \(U_{\tau_i}\) and the interpolation rate \(\beta\) and see what update
		step Reptile would take.
		</p>
		<figure>
			<d-figure id="reptileInterpolation"></d-figure>
		</figure>
		<p>
			By the way, one detail of Reptile that isn't obvious just from looking at the three steps we posted above,
			but still important to note,
			is that Reptile does not (need to) differentiate between test- and training-sets when computing loss
			\(\mathcal{L}_{\tau_i}\) since
			Reptile does not update according to test-set-performance.
		</p>
		<p>
			Furthermore, it makes sense to spend some time studying why the authors of Reptile expilictly state that
			optimizer \(U_{\tau_i}\) must perform
			more than one gradient descent step (\(k > 1\)). This is because otherwise

			\[ U_{\tau_i}(\theta) = \theta - \alpha \nabla_\theta L_{\tau_i}(\theta) \]

			and Reptile updates

			\[ \theta = \theta + \beta (\theta - \alpha \nabla_\theta L_{\tau_i}(\theta) - \theta)
			= \theta - \alpha \beta \nabla_\theta L_{\tau_i}(\theta), \]

			which corresponds to updating \(\theta \) according to standard gradient descent with learning rate \(\alpha
			\cdot \beta\).
			And this in turn is more or less the update scheme we used for the <i>pretrained</i> model, which we have
			already seen to fail.
		</p>
		<p>
			You might have already figured this out yourself, if you set the inner steps of <a
				href="#reptileInterpolation">the figure from above</a>
			to \(1\) (which were set to \(2\) by default deliberately - and now you know why).

			It should however also be noted, as the Nichol et al. state, that as soon as \(k > 1\), the update step
			cannot be reduced to simple gradient descent anymore, i.e.
			it involves terms accounting for meta-performance.
		</p>
		<p>
			Now, at this point you might have already understood <i>how</i> the Reptile update works, but no idea
			<i>if</i> and <i>why</i> it would
			find the same optimal initialization that MAML does! As for the <i>if</i>, Reptile does not (always) find
			the same optimal initialization
			that MAML would find, since Reptile it is not minimizing the same objective. However, Reptile performs
			competitively well compared to MAML in several few-shot learning
			problems.
		</p>
		<p>
			As for the <i>why</i> we would like to refer you to the <a
				href="https://arxiv.org/pdf/1803.02999.pdf">Reptile paper</a> itself.
			Section 5.1 contains a theoretical analysis of how both MAML and Reptile find updates for one task that also
			improve meta-performance for other tasks of the task distribution.
		</p>
		<p>
			After having seen all those extensions, in the next section we would like to compare them visually.
		</p>
		<h3>Comparing the First-Order Methods to MAML</h3>
		<p>
			In order to compare the above methods visually, we return to the \(\textbf{elu}\)-fitting problem from
			before, see <a href="#metaGradient">this figure</a>. This time, however,
			in addition to the task gradient descent directions \(\phi_0\) and \(\phi_1\), we will also plot a single
			update direction of MAML, FOMAML and Reptile.
			Furthermore, you can now also see the <i>combined loss space</i> of the two tasks, such that you can verify
			whether the methods actually would converge
			to the local optimum. The combined loss space is defined via the meta loss of the two tasks, i.e.

			\[\mathcal{L}(\theta) := \sum_{i \in \{0, 1\}}
			\mathcal{L}_{\tau_i, \text{test}}(\phi_i).\]

			A word of warning: The update directions are computed on actual data and with the actual algorithms, running
			in you browser on <a href="https://www.tensorflow.org/js">tensorflow.js</a>.
			If you are experiencing delays on the vector update when moving \(\theta\) you can disable some of the
			computations via
			the panel under the figure.
		</p>
		<figure class="l-body">
			<d-figure id="metaGradient">
				<div class="element is-loading" style="height: 400px"></div>
			</d-figure>
		</figure>

		<h2 id="section_imaml">iMAML: Implicit Gradients</h2>


		<p>
			To explain how Implicit Model-Agnostic Meta-Learning (iMAML) works,
			we will start with an observation:
			If we do many gradient steps in regular MAML (apart from a large
			computational burden), we face the issue that the model-parameters
			\( \phi \) depend less and less on the meta-paramer \( \theta \).
			If the parameters (\( \phi \) and \( \theta \)) are largely independent,
			placing \( \theta \) becomes more difficult, since its effect on \( \phi \)
			diminishes.
		</p>

		<p>
			Regular MAML mitigates this by using only few gradient steps. This early
			stopping is equivalent to a Bayesian prior.
			<dt-cite key="grant_recasting_2018"></dt-cite>

			iMAML utilizes a more explicit regularization.
			<dt-cite key="rajeswaran_meta-learning_2019"></dt-cite>
		</p>

		<p>
			Let's take another look at problem formulation. The objective in
			meta-learning is to minimize the expected loss over a task distribution:

			$$
			\min_\theta \mathbb E_\tau \left[ \mathcal L \left(
			\phi_\tau , \mathcal D ^ {test}_\tau
			\right)\right]
			$$

			Here \( \phi_\tau \) is the task parameter that we acquire after solving
			the inner optimization problem, \( \mathcal D ^ {test}_\tau \) is the
			task-level test dataset.
		</p>

		<p>
			In regular MAML \( \phi_\tau \) is obtained by Computing
			a single update (or a few) gradient descent steps:

			$$
			\phi_\tau = U_\tau (\theta) = \theta - \alpha \nabla_\theta \mathcal L \left( \theta, \mathcal
			D^{train}_\theta \right)
			$$
		</p>

		<p>
			Now, instead of using only few update steps, we will use an abitrary
			optimizer which optimizes the task-parameter until it reaches a minimum,
			but instead of only minimizing the task-loss we add a \( L_2 \) normalization term.
			term:

			$$
			\phi_\tau = U^\ast_\tau (\theta)
			= \arg\min_\phi \left( \mathcal L \left( \phi, \mathcal D^{train}_\tau \right)
			+ \frac{\lambda}{2} \| \phi - \theta \| ^ 2 \right)
			$$

			Here, the objective is the almost the same, with the addition, that
			the moving to far away from the meta-parameter \( \theta \)
			results in higher loss.
		</p>

		<figure class="l-body-outset">
			<d-figure id="imamlLoss">
				<div class="element is-loading" style="height: 400px"></div>
			</d-figure>
			<figcaption>
				Play around with \( \theta \) and \( \lambda \) to get a feeling for the
				resulting loss space of the inner optimization objective.
				High \( \lambda \) will encourage the the algorithm to place the task-parameter
				close to the meta-parameter \( \theta \). \( \lambda = 0\) results in the
				original loss function.
			</figcaption>
		</figure>


		<h3>Computing the Gradient</h3>

		<p>
			Now, in order to minimize the meta-objective, we again calculate the gradient:

			$$
			\begin{align}
			\nabla_\theta\, \mathbb E_\tau \left[ \mathcal L \left(
			{\phi}_\tau , \mathcal D ^ {test}_\tau
			\right)\right]
			&= \mathbb E_\tau \left[ \nabla_{\theta}\, \mathcal L \left(
			{\phi}_\tau , \mathcal D ^ {test}_\tau
			\right)\right]\\
			&= \mathbb E_\tau \left[ \nabla_{\phi_\tau} \, \mathcal L \left(
			\phi_\tau , \mathcal D ^ {test}_\tau\right) \cdot \frac{\mathrm d \phi_\tau}{\mathrm d \theta}
			\right]
			\end{align}
			$$

			Calculating the first part \( \nabla_{\phi_\tau} \, \mathcal L \left(
			\phi_\tau , \mathcal D ^ {test}_\tau\right) \) can be done using back-propagation.
			This is the gradient of \(\phi_\tau\) at the parameter which was found by
			the optimizer. \( \frac{\mathrm d \phi_\tau}{\mathrm d \theta} \) is the part
			that MAML has its problems with: depending on how complex the optimization
			algorithm is, this may be difficult to compute using back-propagation.
		</p>

		<p>
			But, here comes the awesome part: Since we assume that out inner optimizer
			found a local minimum, we know that the gradient of the inner objective
			in regard to the task-parameter is 0. This gives us the following equation:

			$$
			\begin{align}
			\mathbf {0} &= \nabla_{\phi} \left(
			\mathcal L \left( \phi, \mathcal D^{train} \right)
			+ \frac{\lambda}{2} \| \phi - \theta \| ^ 2

			\right)\\
			&= \nabla_{\phi} \mathcal L \left( \phi, \mathcal D^{train} \right) + \lambda \left( \phi - \theta \right)
			\end{align}
			$$

			By rearranging we get:

			$$
			\phi = \theta - \frac{1}{\lambda} \nabla_\phi \mathcal L \left( \phi, \mathcal D^{train} \right)
			$$
		</p>
		<figure class="l-body-outset">
			<d-figure id="imamlGradient">
				<div class="element is-loading" style="height: 400px"></div>
			</d-figure>
			<figcaption>
				<p>
					The red arrow denotes the gradient \( \nabla_\phi \mathcal L \left( \phi, \mathcal D^{train} \right) \). The gradient
					pulls the task parameters \( \phi \) towards the minimum of the task loss.
					You can imagine the green arrow as being the counter-force whicht pulls
					\( \phi \) toward the meta-parameter \( \theta \).
				</p>
				<p>
					At the optimium, these forces need to cancel out, since moving in any direction
					won't improve the regularized loss.
					Hence, the gradient need to be orthogonal to the isocurve (white circle):
					moving along won't change the regularization term; since \( \phi \) is optimal
					for the joint term, the projection of the task-loss gradient onto the
					circle must be zero (moving some distance along the circle would improve the joint loss).
				</p>

			</figcaption>
		</figure>
		<p>

			Now, we can calculate the Jacobian of the task-parameter \( \phi \) in regard to the meta-parameter  \( \theta \):

			$$
			\begin{align}
			\frac{\mathrm d \phi}{\mathrm d \theta}
			&= \frac{\mathrm d }{\mathrm d \theta} \left( \theta - \frac{1}{\lambda} \nabla_\phi \mathcal L \left( \phi, \mathcal D^{train}
			\right) \right)\\
			&= \frac{\mathrm d \theta}{\mathrm d \theta} - \frac{1}{\lambda}\frac{ \mathrm d }{\mathrm d \theta} \nabla_\phi \mathcal L \left(
			\phi, \mathcal D^{train} \right)\\

			%&= \frac{\mathrm d \theta}{\mathrm d \theta} - \frac{1}{\lambda}\frac{ \mathrm d }{\mathrm d \theta} \frac{\mathrm d}{\mathrm d \phi} \mathcal L \left(
			%\phi, \mathcal D^{train} \right)\\

			%&= \frac{\mathrm d \theta}{\mathrm d \theta} - \frac{1}{\lambda}\frac{ \mathrm d \phi}{\mathrm d \theta} \frac{\mathrm d^2}{\mathrm d \phi ^2} \mathcal L \left(
			%\phi, \mathcal D^{train} \right)\\

			&= I - \frac{1}{\lambda} \nabla^2_\phi \mathcal L \left( \phi, \mathcal D^{train} \right) \frac{\mathrm d
			\phi}{\mathrm d \theta}
			\end{align}
			$$

			Here, to get from the 2nd to 3rd line, we need to apply the chain rule: \( \phi \)
			is a function of \( \theta \). Hence, we need to calculate the outer derivative (which results in the
			Hessian)
			and the inner derivative which is exactly what we want to calculate (also the total derivative \(
			\frac{\mathrm d \phi}{\mathrm d \theta} \)).

			Finally, we arrive at the following equations (assuming the inverse exists):

			$$\begin{align}
			&&(I + \frac{1}{\lambda} \nabla^2_\phi \mathcal L \left( \phi, \mathcal D^{train} \right))\frac{\mathrm d
			\phi}{\mathrm d \theta} = I\\
			\Rightarrow&& \frac{\mathrm d \phi}{\mathrm d \theta} = (I + \frac{1}{\lambda} \nabla^2_\phi \mathcal L
			\left( \phi, \mathcal D^{train} \right))^{-1}
			\end{align}$$

			Let that sink in for a moment: By assuming that our inner optimizer
			found an optimal solution for our inner objective, we can derive a closed-form
			solution for the total derivative \( \frac{\mathrm d \phi}{\mathrm d \theta} \).
			To calculate the meta-gradient, we just need to know the solution
			of the inner optimizaton problem, without knowing the steps to get there!
		</p>

		<p>
			The steps leading up to the optimal solution do not need to be stored and
			we could even use an optimizer which does not use Gradient Descent and
			where we might not be able to back-propagate the gradient. Instead,
			the optimizer can be treated as a black-box and we only require
			the final solution.
		</p>

		<p>
			If this implicit differatiation is interesting to you, take a look at the
			paper "Efficient and Modular Implicit Differentiation". In this paper
			the authors offer a more general framework of how to compute gradients
			without needing to backpropagate through the unrolled forward propagation.
			Instead they use an optimiality condition (in the iMAML case its given by
			the gradient of the inner loop objective) in order to calculate the
			gradient implicitly.

			<dt-cite key="blondel_efficient_2021"></dt-cite>

		</p>

		<h3>Discussion</h3>

		<p>
			As you can see in the gradient above, iMAML requires the computation of
			a second-order derivative. The huge benefit is that this second-order
			derivative only needs to be calculated for the last point the optimizer
			calculated. We do not need to pass the gradient information through
			the steps of gradient-descent.
		</p>

		<p>
			While calculating the gradient is comparitively easy, iMAML requires
			an optimizer which finds a quasi-optimal solution. Even though we introduced
			the math behind iMAML with the assumption that the optimizer finds an optimal
			solution, the authors of "Meta-Learning with Implicit Gradients" show
			that the gradient is still approximately correct as long as the solution
			provided by the inner-loop optimizer is approximately correct.
			Still, we probably need to make many more gradient steps (if we use SGD),
			then in regular MAML where even one step may suffice.
			<dt-cite key="rajeswaran_meta-learning_2019"></dt-cite>
		</p>

		<p>
			According to the same paper, iMAML produces better results then MAML, while
			not using more resources (while iMAML requires more inner loop steps),
			MAML requires either more outer loops steps or expensive computation
			of a long back-propagation chain. Compared to first-order MAML (FOMAML)
			and REPTILE, the authors report better results on the
			<dt-cite key="lake_omniglot_2015">Omniglot</dt-cite>
			(the little exercise at the top of the page is base on Omniglot)
			and
			<dt-cite key="liu_tools_2019">Mini-ImageNet</dt-cite>,
			two common few-shot classification datasets.
			<dt-cite key="rajeswaran_meta-learning_2019"></dt-cite>
		</p>

		<p>
			As Ferenc Husz√°r points out in his wonderful
			<a href="https://www.inference.vc/notes-on-imaml-meta-learning-without-differentiating-through/#what-is-missing-stochasticity"
				alt="Husz√°r's blog post on iMAML">blog post on iMAML</a>,
			iMAML does not consider the stochasticity of Stochastic Gradient-Descent:
			SGD may have non-zero proabilities of finding more then one task-level
			optimum, but iMAML will only derive the gradient in respect to actually
			found optimum.
		</p>
		<p>
			If you are interested int this condideration, you may want to also take
			a look at the paper titled "Probabilistic Model-Agnostic Meta-Learning".
			<dt-cite key="finn_probabilistic_2019"></dt-cite>
		</p>

	</dt-article>

	<dt-appendix>
	</dt-appendix>


	<script type="text/bibliography">
@misc{finn2017modelagnostic,
    title={Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks},
    author={Chelsea Finn and Pieter Abbeel and Sergey Levine},
    year={2017},
    eprint={1703.03400},
    archivePrefix={arXiv},
    primaryClass={cs.LG},
		url={https://arxiv.org/abs/1703.03400},
}


@article{lake_one_2011,
	title = {One shot learning of simple visual concepts},
	abstract = {People can learn visual concepts from just one example, but it remains a mystery how this is accomplished. Many authors have proposed that transferred knowledge from more familiar concepts is a route to one shot learning, but what is the form of this abstract knowledge? One hypothesis is that the sharing of parts is core to one shot learning, and we evaluate this idea in the domain of handwritten characters, using a massive new dataset. These simple visual concepts have a rich internal part structure, yet they are particularly tractable for computational models. We introduce a generative model of how characters are composed from strokes, where knowledge from previous characters helps to infer the latent strokes in novel characters. The stroke model outperforms a competing stateof-the-art character model on a challenging one shot learning task, and it provides a good Ô¨Åt to human perceptual data.},
	language = {en},
	author = {Lake, Brenden M and Salakhutdinov, Ruslan and Gross, Jason and Tenenbaum, Joshua B},
	year = {2011},
	url = {https://cims.nyu.edu/~brenden/LakeEtAl2011CogSci.pdf},
}


@article{lake_one-shot_2013,
	title = {One-shot learning by inverting a compositional causal process},
	abstract = {People can learn a new visual class from just one example, yet machine learning algorithms typically require hundreds or thousands of examples to tackle the same problems. Here we present a Hierarchical Bayesian model based on compositionality and causality that can learn a wide range of natural (although simple) visual concepts, generalizing in human-like ways from just one image. We evaluated performance on a challenging one-shot classiÔ¨Åcation task, where our model achieved a human-level error rate while substantially outperforming two deep learning models. We also tested the model on another conceptual task, generating new examples, by using a ‚Äúvisual Turing test‚Äù to show that our model produces human-like performance.},
	language = {en},
	author = {Lake, Brenden M and Salakhutdinov, Ruslan},
	year = {2013},
	url = {https://cims.nyu.edu/~brenden/LakeEtAlNips2013.pdf}
}



@article{koch_siamese_2015,
	title = {Siamese Neural Networks for One-shot Image Recognition},
	abstract = {The process of learning good features for machine learning applications can be very computationally expensive and may prove difÔ¨Åcult in cases where little data is available. A prototypical example of this is the one-shot learning setting, in which we must correctly make predictions given only a single example of each new class. In this paper, we explore a method for learning siamese neural networks which employ a unique structure to naturally rank similarity between inputs. Once a network has been tuned, we can then capitalize on powerful discriminative features to generalize the predictive power of the network not just to new data, but to entirely new classes from unknown distributions. Using a convolutional architecture, we are able to achieve strong results which exceed those of other deep learning models with near state-of-the-art performance on one-shot classiÔ¨Åcation tasks.},
	language = {en},
	author = {Koch, Gregory and Zemel, Richard and Salakhutdinov, Ruslan},
	year = {2015},
	url = {https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf},
}


@article{vinyals_matching_2017,
	title = {Matching Networks for One Shot Learning},
	url = {http://arxiv.org/abs/1606.04080},
	abstract = {Learning from a few examples remains a key challenge in machine learning. Despite recent advances in important domains such as vision and language, the standard supervised deep learning paradigm does not offer a satisfactory solution for learning new concepts rapidly from little data. In this work, we employ ideas from metric learning based on deep neural features and from recent advances that augment neural networks with external memories. Our framework learns a network that maps a small labelled support set and an unlabelled example to its label, obviating the need for fine-tuning to adapt to new class types. We then define one-shot learning problems on vision (using Omniglot, ImageNet) and language tasks. Our algorithm improves one-shot accuracy on ImageNet from 87.6\% to 93.2\% and from 88.0\% to 93.8\% on Omniglot compared to competing approaches. We also demonstrate the usefulness of the same model on language modeling by introducing a one-shot task on the Penn Treebank.},
	urldate = {2021-06-12},
	author = {Vinyals, Oriol and Blundell, Charles and Lillicrap, Timothy and Kavukcuoglu, Koray and Wierstra, Daan},
	month = dec,
	year = {2017},
	note = {arXiv: 1606.04080},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning}
}


@article{santoro_meta-learning_2016,
	title = {Meta-Learning with Memory-Augmented Neural Networks},
	abstract = {Despite recent breakthroughs in the applications of deep neural networks, one setting that presents a persistent challenge is that of ‚Äúone-shot learning.‚Äù Traditional gradient-based networks require a lot of data to learn, often through extensive iterative training. When new data is encountered, the models must inefÔ¨Åciently relearn their parameters to adequately incorporate the new information without catastrophic interference. Architectures with augmented memory capacities, such as Neural Turing Machines (NTMs), offer the ability to quickly encode and retrieve new information, and hence can potentially obviate the downsides of conventional models. Here, we demonstrate the ability of a memory-augmented neural network to rapidly assimilate new data, and leverage this data to make accurate predictions after only a few samples. We also introduce a new method for accessing an external memory that focuses on memory content, unlike previous methods that additionally use memory locationbased focusing mechanisms.},
	language = {en},
	author = {Santoro, Adam and Bartunov, Sergey and Botvinick, Matthew and Wierstra, Daan and Lillicrap, Timothy},
	year = {2016},
	url = {https://web.stanford.edu/class/psych209/Readings/Santoro16MetaLearningWithMemAugNNs.pdf},
}



@article{andrychowicz_learning_2016,
	title = {Learning to learn by gradient descent by gradient descent},
	url = {http://arxiv.org/abs/1606.04474},
	abstract = {The move from hand-designed features to learned features in machine learning has been wildly successful. In spite of this, optimization algorithms are still designed by hand. In this paper we show how the design of an optimization algorithm can be cast as a learning problem, allowing the algorithm to learn to exploit structure in the problems of interest in an automatic way. Our learned algorithms, implemented by LSTMs, outperform generic, hand-designed competitors on the tasks for which they are trained, and also generalize well to new tasks with similar structure. We demonstrate this on a number of tasks, including simple convex problems, training neural networks, and styling images with neural art.},
	urldate = {2021-06-12},
	author = {Andrychowicz, Marcin and Denil, Misha and Gomez, Sergio and Hoffman, Matthew W. and Pfau, David and Schaul, Tom and Shillingford, Brendan and de Freitas, Nando},
	month = nov,
	year = {2016},
	note = {arXiv: 1606.04474},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}



@article{snell_prototypical_2017,
	title = {Prototypical Networks for Few-shot Learning},
	url = {http://arxiv.org/abs/1703.05175},
	abstract = {We propose prototypical networks for the problem of few-shot classiÔ¨Åcation, where a classiÔ¨Åer must generalize to new classes not seen in the training set, given only a small number of examples of each new class. Prototypical networks learn a metric space in which classiÔ¨Åcation can be performed by computing distances to prototype representations of each class. Compared to recent approaches for few-shot learning, they reÔ¨Çect a simpler inductive bias that is beneÔ¨Åcial in this limited-data regime, and achieve excellent results. We provide an analysis showing that some simple design decisions can yield substantial improvements over recent approaches involving complicated architectural choices and meta-learning. We further extend prototypical networks to zero-shot learning and achieve state-of-theart results on the CU-Birds dataset.},
	language = {en},
	urldate = {2021-06-11},
	author = {Snell, Jake and Swersky, Kevin and Zemel, Richard S.},
	month = jun,
	year = {2017},
	note = {arXiv: 1703.05175},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}



@article{munkhdalai_meta_2017,
	title = {Meta Networks},
	url = {http://arxiv.org/abs/1703.00837},
	abstract = {Neural networks have been successfully applied in applications with a large amount of labeled data. However, the task of rapid generalization on new concepts with small training data while preserving performances on previously learned ones still presents a significant challenge to neural network models. In this work, we introduce a novel meta learning method, Meta Networks (MetaNet), that learns a meta-level knowledge across tasks and shifts its inductive biases via fast parameterization for rapid generalization. When evaluated on Omniglot and Mini-ImageNet benchmarks, our MetaNet models achieve a near human-level performance and outperform the baseline approaches by up to 6\% accuracy. We demonstrate several appealing properties of MetaNet relating to generalization and continual learning.},
	urldate = {2021-06-12},
	author = {Munkhdalai, Tsendsuren and Yu, Hong},
	month = jun,
	year = {2017},
	note = {arXiv: 1703.00837},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Accepted at ICML 2017 - rewrote: the main section; added: MetaNet algorithmic procedure; performed: Mini-ImageNet evaluation},
}




@article{andrychowicz_learning_2016,
	title = {Learning to learn by gradient descent by gradient descent},
	url = {http://arxiv.org/abs/1606.04474},
	abstract = {The move from hand-designed features to learned features in machine learning has been wildly successful. In spite of this, optimization algorithms are still designed by hand. In this paper we show how the design of an optimization algorithm can be cast as a learning problem, allowing the algorithm to learn to exploit structure in the problems of interest in an automatic way. Our learned algorithms, implemented by LSTMs, outperform generic, hand-designed competitors on the tasks for which they are trained, and also generalize well to new tasks with similar structure. We demonstrate this on a number of tasks, including simple convex problems, training neural networks, and styling images with neural art.},
	urldate = {2021-06-12},
	journal = {arXiv:1606.04474 [cs]},
	author = {Andrychowicz, Marcin and Denil, Misha and Gomez, Sergio and Hoffman, Matthew W. and Pfau, David and Schaul, Tom and Shillingford, Brendan and de Freitas, Nando},
	month = nov,
	year = {2016},
	note = {arXiv: 1606.04474},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing}
}


@article{rajeswaran_meta-learning_2019,
	title = {Meta-Learning with Implicit Gradients},
	url = {http://arxiv.org/abs/1909.04630},
	abstract = {A core capability of intelligent systems is the ability to quickly learn new tasks by drawing on prior experience. Gradient (or optimization) based meta-learning has recently emerged as an effective approach for few-shot learning. In this formulation, meta-parameters are learned in the outer loop, while task-specific models are learned in the inner-loop, by using only a small amount of data from the current task. A key challenge in scaling these approaches is the need to differentiate through the inner loop learning process, which can impose considerable computational and memory burdens. By drawing upon implicit differentiation, we develop the implicit MAML algorithm, which depends only on the solution to the inner level optimization and not the path taken by the inner loop optimizer. This effectively decouples the meta-gradient computation from the choice of inner loop optimizer. As a result, our approach is agnostic to the choice of inner loop optimizer and can gracefully handle many gradient steps without vanishing gradients or memory constraints. Theoretically, we prove that implicit MAML can compute accurate meta-gradients with a memory footprint that is, up to small constant factors, no more than that which is required to compute a single inner loop gradient and at no overall increase in the total computational cost. Experimentally, we show that these benefits of implicit MAML translate into empirical gains on few-shot image recognition benchmarks.},
	urldate = {2021-04-30},
	journal = {arXiv:1909.04630 [cs, math, stat]},
	author = {Rajeswaran, Aravind and Finn, Chelsea and Kakade, Sham and Levine, Sergey},
	month = sep,
	year = {2019},
	note = {arXiv: 1909.04630},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Optimization and Control},
}

@article{grant_recasting_2018,
	title = {Recasting Gradient-Based Meta-Learning as Hierarchical Bayes},
	url = {http://arxiv.org/abs/1801.08930},
	abstract = {Meta-learning allows an intelligent agent to leverage prior learning episodes as a basis for quickly improving performance on a novel task. Bayesian hierarchical modeling provides a theoretical framework for formalizing meta-learning as inference for a set of parameters that are shared across tasks. Here, we reformulate the model-agnostic meta-learning algorithm (MAML) of Finn et al. (2017) as a method for probabilistic inference in a hierarchical Bayesian model. In contrast to prior methods for meta-learning via hierarchical Bayes, MAML is naturally applicable to complex function approximators through its use of a scalable gradient descent procedure for posterior inference. Furthermore, the identification of MAML as hierarchical Bayes provides a way to understand the algorithm's operation as a meta-learning procedure, as well as an opportunity to make use of computational strategies for efficient inference. We use this opportunity to propose an improvement to the MAML algorithm that makes use of techniques from approximate inference and curvature estimation.},
	urldate = {2021-06-30},
	journal = {arXiv:1801.08930 [cs]},
	author = {Grant, Erin and Finn, Chelsea and Levine, Sergey and Darrell, Trevor and Griffiths, Thomas},
	month = jan,
	year = {2018},
	note = {arXiv: 1801.08930},
	keywords = {Computer Science - Machine Learning},
}


@article{blondel_efficient_2021,
	title = {Efficient and Modular Implicit Differentiation},
	url = {http://arxiv.org/abs/2105.15183},
	abstract = {Automatic differentiation (autodiff) has revolutionized machine learning. It allows expressing complex computations by composing elementary ones in creative ways and removes the burden of computing their derivatives by hand. More recently, differentiation of optimization problem solutions has attracted widespread attention with applications such as optimization as a layer, and in bi-level problems such as hyper-parameter optimization and meta-learning. However, the formulas for these derivatives often involve case-by-case tedious mathematical derivations. In this paper, we propose a uniÔ¨Åed, efÔ¨Åcient and modular approach for implicit differentiation of optimization problems. In our approach, the user deÔ¨Ånes (in Python in the case of our implementation) a function F capturing the optimality conditions of the problem to be differentiated. Once this is done, we leverage autodiff of F and implicit differentiation to automatically differentiate the optimization problem. Our approach thus combines the beneÔ¨Åts of implicit differentiation and autodiff. It is efÔ¨Åcient as it can be added on top of any state-of-the-art solver and modular as the optimality condition speciÔ¨Åcation is decoupled from the implicit differentiation mechanism. We show that seemingly simple principles allow to recover many recently proposed implicit differentiation methods and create new ones easily. We demonstrate the ease of formulating and solving bi-level optimization problems using our framework. We also showcase an application to the sensitivity analysis of molecular dynamics.},
	language = {en},
	urldate = {2021-07-03},
	journal = {arXiv:2105.15183 [cs, math, stat]},
	author = {Blondel, Mathieu and Berthet, Quentin and Cuturi, Marco and Frostig, Roy and Hoyer, Stephan and Llinares-L√≥pez, Felipe and Pedregosa, Fabian and Vert, Jean-Philippe},
	month = may,
	year = {2021},
	note = {arXiv: 2105.15183},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Numerical Analysis},
}

@article{finn_probabilistic_2019,
	title = {Probabilistic Model-Agnostic Meta-Learning},
	url = {http://arxiv.org/abs/1806.02817},
	abstract = {Meta-learning for few-shot learning entails acquiring a prior over previous tasks and experiences, such that new tasks be learned from small amounts of data. However, a critical challenge in few-shot learning is task ambiguity: even when a powerful prior can be meta-learned from a large number of prior tasks, a small dataset for a new task can simply be too ambiguous to acquire a single model (e.g., a classifier) for that task that is accurate. In this paper, we propose a probabilistic meta-learning algorithm that can sample models for a new task from a model distribution. Our approach extends model-agnostic meta-learning, which adapts to new tasks via gradient descent, to incorporate a parameter distribution that is trained via a variational lower bound. At meta-test time, our algorithm adapts via a simple procedure that injects noise into gradient descent, and at meta-training time, the model is trained such that this stochastic adaptation procedure produces samples from the approximate model posterior. Our experimental results show that our method can sample plausible classifiers and regressors in ambiguous few-shot learning problems. We also show how reasoning about ambiguity can also be used for downstream active learning problems.},
	urldate = {2021-04-30},
	journal = {arXiv:1806.02817 [cs, stat]},
	author = {Finn, Chelsea and Xu, Kelvin and Levine, Sergey},
	month = oct,
	year = {2019},
	note = {arXiv: 1806.02817
version: 2},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: NeurIPS 2018. First two authors contributed equally. Supplementary results available at https://sites.google.com/view/probabilistic-maml/},
}



@misc{lake_omniglot_2015,
	title = {Omniglot data set for one-shot learning},
	url = {https://github.com/brendenlake/omniglot},
	abstract = {The Omniglot data set is designed for developing more human-like learning algorithms. It contains 1623 different handwritten characters from 50 different alphabets. Each of the 1623 characters was drawn online via Amazon's Mechanical Turk by 20 different people. Each image is paired with stroke data, a sequences of [x,y,t] coordinates with time (t) in milliseconds.},
	urldate = {2021-07-07},
	journal = {Omniglot data set for one-shot learning},
	author = {Lake, Brenden M and Salakhutdinov, Ruslan and Tenenbaum, Joshua B},
	month = oct,
	year = {2015},
}

@misc{liu_tools_2019,
	title = {Tools for mini-ImageNet Dataset},
	journal = {Tools for mini-ImageNet Dataset},
	author = {Liu, Yaoyao},
	month = jan,
	year = {2019},
	url = {https://github.com/yaoyao-liu/mini-imagenet-tools}
}



</script>
</body>

</html>
