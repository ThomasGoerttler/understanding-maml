<!DOCTYPE html>
<html>
	<head>
		<title>Visual Comparison of Model-Agnostic Meta-Learning üë©‚Äçüî¨</title>
		<meta charset="UTF-8"/>
		<link rel="stylesheet" href="build/bundle.css">
  </head>
  <body>
    <script async src="https://distill.pub/template.v1.js"></script>
    <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>
    <script defer src="build/bundle.js"></script>

<script type="text/front-matter">
  title: "Visual Comparison of Model-Agnostic Meta-Learning"
  description: "t.b.d."
  authors:
  - Luis M√ºller: https://github.com/pupuis
  - Max Ploner:
  affiliations:
  - NI @ TU Berlin: https://www.ni.tu-berlin.de/menue/neural_information_processing_group/
  - NI @ TU Berlin: https://www.ni.tu-berlin.de/menue/neural_information_processing_group/
</script>

<dt-article>
	<h1>A Visual Comparison of Model-Agnostic Meta-Learning</h1>
	<h2>Exploring the world of model-agnostic meta-learning and its extensions.</h2>
	<figure class="l-page">
    <d-figure id="teaser">
			<div class="element is-loading" style="height: 400px"></div>
		</d-figure>
		<figcaption>
			Click button or drag sample onto example to classify the sample.
		</figcaption>
  </figure>

	<dt-byline></dt-byline>

	<p>
		If you tried the little exercise above, you probably got a pretty good score.
		Even though you likely have never seen some of the characters,
		you are able to compare them to a another one, potentially without realizing
		that what you are doing is actually pretty impressive!
	</p>

	<figure class="l-page side">
    <d-figure id="fewShotMethods">
			<div class="element is-loading" style="height: 400px"></div>
		</d-figure>
		<figcaption>
			<p>Results of different methods on the omniglot dataset.</p>
			<ul>
				<li><span id="generative_stroke_model" class="fewShotMethods-reference">
						The generative stroke model was introduced in the paper which also
						introduced omniglot. The model is based on a latent stroke representation
						(including the number and directions of strokes). While it is an
						interesting approach, it can hardly be generalized to other
						few-shot problems.
				</span></li>
			</ul>
		</figcaption>
  </figure>

	<p>
		Typically, in machine learning we need a lot of examples to be able to
		classify a class. But not always do we have enough data to cater this need:
		a sufficient amount of data may be expensive or even impossible to acquire.
		But there is good reasons to believe, that this is not an inherent issue of learning.
		Humans, for example, are quite good at generalizing after seeing only a few
		samples. Enabling machine learning methods to achieve the same would allow
		for many new applications and generally reduce the need to collect huge
		datasets.[<b>reference needed</b>]
	</p>

	<p>
		This research area is called "few-shot learning". It aims to enable models
		to classify unseen sampels after training only one a few examples ("shots").
		The little exercise at the top of the article is an exmaple of such a task.
		The symbols are part of a dataset called "omniglot". <dt-cite key="lake_one_2011"></dt-cite>
		It contains 1623 different characters across 50 alphabets and each character
		has 20 instances drawn by different people. Because of that, the omniglot
		was described as a "transpose" of the well-known MNIST dataset, by the authors
		who originally proposed omniglot as a machine learning dataset. <dt-cite key="lake_one_2011"></dt-cite>
		While MNIST contains only a few classes (the digits 0 to 9) and many instances,
		of each class, with omniglot it is vice-versa.
	</p>


	<p><strong>[...]</strong></p>

  <h2>Few-shot learning with MAML</h2>
  <p>To get familiar with the problem and also to motivate the need for meta-learning we start with something you probably already know
    and develop it to something new. If you have done machine learning before you have probably already solved or attempted to solve a
    trivial special case of meta-learning: Learning a model to solve one specific task, for example to classify images or to teach an agent
    to find its way through a maze. In those settings, if we are able to define a loss \(\mathcal{L}_\tau\) for our task \(\tau\) which depends on the parameters
    \(\phi\) of a model, we can express our learning objective as

    \[ \underset{\phi}{\text{min}} \, \mathcal{L}_\tau (\phi) .\]

    We usually find the optimal \(\phi\) by progressively walking along the direction of the gradient of
    \(\mathcal{L}_\tau\) with respect to \(\phi\), i.e.

    \[ \phi \leftarrow \phi - \alpha \nabla_\phi \mathcal{L}_\tau (\phi) ,\]

    also known as gradient descent, where \(\mathcal{L}_\tau\) usually also depends on some data and \(\alpha\) is a fixed learning rate,
    controlling the size of the steps we want to take.
  </p>
  <p>We will now make an additional assumption, namely that \(\tau\) comes from some distribution of tasks \(p(\tau)\) and that we
    can sample freely from this distribution. We can then generalize the above objective to learn how to find an
    optimization strategy for a randomly sampled task from \(p(\tau)\). If we additionally ensure that the optimization strategy requires observing only
    a few samples from this task, we
    would have come up with a solution to few-shot learning for tasks from \(p(\tau)\). We
    can express our new learning objective as follows:

    \[ \text{min} \, \mathbb{E}_\tau [ \mathcal{L}_\tau (\phi_\tau) ] ,\]

    where \(\tau\) is now a random variable and \(\phi_\tau\) are a set of parameters for task \(\tau\), which is nothing more than a practical way to formalize really.
    We may use different parameters for each task, use the same parameters for every task or something in between. But anyway, how
    are we actually minimizing this new objective? In the following we will explore two possible answers to this question.
  </p>
  <h3>Answer 1: Just do gradient descent, silly!</h3>
  <p>Of course! The answer to most problems. We can simply learn a single set of parameters \(\phi\) which minimizes the expected loss
    over all tasks. Or formally speaking our objective becomes

    \[ \underset{\phi}{\text{min}} \, \mathbb{E}_\tau [ \mathcal{L}_\tau (\phi) ] .\]

    Further, to actually implement this strategy we can do what is known as <i>Expected Risk Minimization (ERM)</i>, which roughly
    dictates us to sample a lot of tasks \(\tau\) and then descent according to

    \[ \phi \leftarrow \phi - \alpha \nabla_\phi \sum_i \mathcal{L}_{\tau_i} (\phi) .\]

    To make this a bit more robust we can also use an optimizer using an adaptive step size, e.g. <i>Adam</i>. In the following figure
    you can experiment with a model that was trained in exactly this way.
  </p>

  <figure>
    <d-figure id="fitSinePretrained"></d-figure>
  </figure>


  <p>If we want to use equations we can either write inline, e.g. saying that we intitalize \(\alpha = 0.01\), or in its own paragraph, e.g. saying we then learn our updates via
    \[ \theta_i' = \theta - \alpha \nabla_\theta \mathcal{L}_{\mathcal{T}_i} (f_\theta) .\]

  You can implement this in python as follows:
	</p>
  <dt-code block language="python">
      def step(theta, alpha, model, data):
          return theta - alpha * model.grad(data)
  </dt-code>

	<figure class="l-body">
    <d-figure id="metaGradient"><div class="element is-loading" style="height: 400px"></div></d-figure>
  </figure>

	<figure class="l-body-outset">
	<d-figure id="userOptimizedTheta"><div class="element is-loading" style="height: 400px"></div></d-figure>
	</figure>

</dt-article>

<dt-appendix>
</dt-appendix>


<script type="text/bibliography">
@misc{finn2017modelagnostic,
    title={Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks},
    author={Chelsea Finn and Pieter Abbeel and Sergey Levine},
    year={2017},
    eprint={1703.03400},
    archivePrefix={arXiv},
    primaryClass={cs.LG},
		url={https://arxiv.org/abs/1703.03400},
}


@article{lake_one_2011,
	title = {One shot learning of simple visual concepts},
	abstract = {People can learn visual concepts from just one example, but it remains a mystery how this is accomplished. Many authors have proposed that transferred knowledge from more familiar concepts is a route to one shot learning, but what is the form of this abstract knowledge? One hypothesis is that the sharing of parts is core to one shot learning, and we evaluate this idea in the domain of handwritten characters, using a massive new dataset. These simple visual concepts have a rich internal part structure, yet they are particularly tractable for computational models. We introduce a generative model of how characters are composed from strokes, where knowledge from previous characters helps to infer the latent strokes in novel characters. The stroke model outperforms a competing stateof-the-art character model on a challenging one shot learning task, and it provides a good Ô¨Åt to human perceptual data.},
	language = {en},
	author = {Lake, Brenden M and Salakhutdinov, Ruslan and Gross, Jason and Tenenbaum, Joshua B},
	year = {2011},
	url = {https://cims.nyu.edu/~brenden/LakeEtAl2011CogSci.pdf},
}

</script>
  </body>
</html>
