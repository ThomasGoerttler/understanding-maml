<!DOCTYPE html>
<html>

<head>
	<title>An Interactive Introduction to Model-Agnostic Meta-Learning üë©‚Äçüî¨</title>
	<meta charset="UTF-8" />
	<link rel="stylesheet" href="build/bundle.css">
</head>

<body>
	<script async src="https://distill.pub/template.v2.js"></script>
	<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

	<script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>
	<script defer src="build/bundle.js"></script>

	<d-front-matter>

		<script type="text/json">
			{
				"title": "An Interactive Introduction to Model-Agnostic Meta-Learning",
				"description": "Exploring the world of model-agnostic meta-learning and its extensions.",
				"authors": [
					{
						"author":"Luis M√ºller",
						"authorURL":"https://github.com/pupuis",
						"affiliations": [
							{"name": "NI @ TU Berlin", "url": "https://www.ni.tu-berlin.de/menue/neural_information_processing_group/"}
						]
					},
					{
						"author":"Max Ploner",
						"authorURL":"https://maxploner.de",
						"affiliations": [
							{"name": "NI @ TU Berlin", "url": "https://www.ni.tu-berlin.de/menue/neural_information_processing_group/"},
							{"name": "ML&nbsp;@&nbsp;HU&nbsp;Berlin", "url": "https://www.informatik.hu-berlin.de/en/forschung-en/gebiete/ml-en"}
						]
					}
				]
			}

		</script>
	</d-front-matter>

	<d-title>
		<h1>An Interactive Introduction to Model-Agnostic Meta-Learning</h1>
		<h2>Exploring the world of model-agnostic meta-learning and its extensions.</h2>
		<figure class="l-page teaser-figure">
			<d-figure id="teaser">
				<div class="element is-loading" style="height: 400px"></div>
			</d-figure>
			<figcaption>
				<p style="font-size: 1.5em; margin-bottom: 1.2rem; margin-top: 1.5em">
					<i><b>MAML</b> learns tasks like the ones above by aciquiring meta-knowledge about similar problems.</i>
				</p>
				<p>
					What you have in front of you is a 5- or 20-way-1-shot problem, one that most conventional machine
					learning
					systems struggle to solve.
					To classify a sample (top), either drag it to or click on the desired class (bottom) and see if you can
					do better. To switch between 5-way and 20-way (which decides how many classes are available), use the
					drop-down menu on the top
					right.
				</p>
			</figcaption>
		</figure>
	</d-title>

	<d-byline></d-byline>

	<d-article>

		<p>
			<i style="font-size: .8em;">
				This page is part of a multi-part series on Model-Agnostic Meta-Learning.
				If you are already familiar with the topic, use the menu on the left
				side to jump straight to the part that is of interest for you. Otherwise,
				we suggest you start at here.
			</i>
		</p>

		<d-contents>
		  <nav class="toc figcaption" id="menu">
		  </nav>
		  <div class="toc-line"></div>
		</d-contents>



		<p>
			If you tried the exercise above, you have undoubtedly received a very high accuracy score.
			Even though you likely have never seen some of the characters,
			you are able to classify them given only a single example, potentially
			without realizing that what you are able to do off the top of your head would be pretty impressive to the
			average
			Deep Neural Net!
		</p>
		<p>
			In this article we would like to give you an interactive introduction to
			Model-Agnostic Meta-Learning,
			a research field
			that attempts to equip conventional machine learning architectures with the power to gain meta-knowledge
			about a range of
			tasks in order to solve problems like the one above on a human level of accuracy.
		</p>

		<h3>Getting Started</h3>
		<p>
			A common wisdom shared across the machine learning community is the need to train models on a truck-load of
			samples before they are able to
			make meaningful predictions on unseen data. But we do not always have enough data available to
			cater this need: A sufficient amount of data may be expensive or even
			impossible to acquire.
			Yet, there are good reasons to believe that this is not an inherent issue
			of learning.
			Humans, such as yourself, are known to excel at generalizing after seeing only a few
			samples <d-cite bibtex-key="salakhutdinov_one-shot_nodate"></d-cite>.
			It should, however, also be noted that humans do not learn
			novel concepts "in a vacuum"<d-cite bibtex-key="lake_one_2011"></d-cite> but based off a lot of prior
			knowledge.
			Enabling machine learning methods to achieve the same would bring us a step closer to learning on the data-
			and energy-efficiency
			level of humans. Consequently we would require algorithms to do the following two things, already
			successfully implemented in humans:
		</p>
		<ul>
			<li>(a) Obtaining as much prior knowledge about the world as possible and</li>
			<li>(b) using that to generalize well on only a few samples.</li>
		</ul>
		<p>
			The method we present in this article has prominently emerged from research
			in two fields which each address one of the above requirements. While introducing these fields to you, we
			will
			also equip you with the most important terms and concepts we will need along the rest of the article.
		</p>

		<h4>(a) Obtaining Prior Knowledge</h4>
		<p>
			While clearly one sample is not enough for a model without prior knowledge,
			we can pretrain models on tasks that we assume to be similar to the target tasks.
			The idea in its core is to derive an inductive bias
			from a set of problem-classes in order to perform better on other, newly encountered, problem-classes.
			This similarity assumption allows the model to collect meta-knowledge:
			knowledge which is not obtained from a single task, but from a distribution of tasks.
			The learning of this meta-knowledge is called "meta-learning".
		</p>

		<h4>(b) Generalization on a Few Samples</h4>
		<p>
			Achieving rapid convergence of machine learning models on but a few samples is known as
			"few-shot learning". If you are presented with \(N\) samples and are expected to learn a classification
			problem with \( M \)
			classes, we speak of an \( M \)-way-\(N\)-shot problem.
			The exercise at the top of the article, which we offer either as a \(20\)- or \(5\)-way-1-shot problem, is a
			prominent example of a few-shot learning task, whose symbols are
			taken from
			the <d-cite bibtex-key="lake_omniglot_2015"><i>omniglot</i> dataset</d-cite>.
			It contains 1623 different characters across 50 alphabets with each character
			being represented by 20 instances, each drawn by a different person. Because of that, the original authors
			of the omniglot dataset
			described it as a "transpose" of the well-known MNIST dataset <d-cite bibtex-key="lake_one_2011"></d-cite>,
			with MNIST containing only a few classes (the digits 0 to 9) and many instances and omniglot containing a
			lot of classes but only a few instances for each.
		</p>



		<p>
			Having set the scene, we can now dig into MAML and its
			extensions. Continue reading on <a href="meta-learning.html">the next page</a>
			to find out why MAML is called
			"model-agnostic",
			go straight to an explanation of <a href="maml.html">MAML</a>,
			or navigate straight to the part you are most interested in using the <a href="#menu">menu at the top</a>.
		</p>

	</d-article>

	<d-appendix>
		<d-bibliography src="references.bib"></d-bibliography>
	</d-appendix>


</body>

</html>
