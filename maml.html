<!DOCTYPE html>
<html>

<head>
	<title>An Interactive Introduction to Model-Agnostic Meta-Learning üë©‚Äçüî¨</title>
	<meta charset="UTF-8" />
	<link rel="stylesheet" href="build/bundle.css">
</head>

<body>
	<script async src="https://distill.pub/template.v1.js"></script>
	<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

	<script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>
	<script defer src="build/bundle.js"></script>

<script type="text/front-matter">
  title: "An Interactive Introduction to Model-Agnostic Meta-Learning"
  description: "t.b.d."
  authors:
  - Luis M√ºller: https://github.com/pupuis
  - Max Ploner:
  affiliations:
  - NI @ TU Berlin: https://www.ni.tu-berlin.de/menue/neural_information_processing_group/
  - NI @ TU Berlin: https://www.ni.tu-berlin.de/menue/neural_information_processing_group/
</script>

	<dt-article>
		<h1>An Interactive Introduction to Model-Agnostic Meta-Learning</h1>
		<h2>Exploring the world of model-agnostic meta-learning and its extensions.</h2>

		<dt-byline></dt-byline>

		<div class="l-middle side">
			<div id="menu">
				<div class="element is-loading" style="height: 400px"></div>
			</div>
		</div>

		<p>
			<i style="font-size: .8em;">
				This page is part of a multi-part series on Model-Agnostic Meta-Learning.
				If you are already familiar with the topic, use the menu on the right
				side to jump straight to the part that is of interest for you. Otherwise,
				we suggest you start at the <a href="./">beginning</a>.
			</i>
		</p>





		<h2 id="few_shot_maml">Few-shot learning with MAML</h2>
		<p>Model-Agnostic Meta Learning (MAML) is a meta-learning approach to solve few-shot learning <dt-cite key="finn2017modelagnostic"></dt-cite>.
			To learn more about it, let us build an example from the ground up and then try to apply MAML.
			We will do this by alternating mathematical walk-throughs and interactive, as well as, coding examples.
		</p>
		<!--TODO: Maybe card to github repo for all code: https://github.com/lepture/github-cards-->
		<p>
			If you have done machine learning
			before you have probably already solved or attempted to solve a problem like the following:
			Learning a model to solve one specific task, for example to classify cats from dogs or to
			teach an agent
			to find its way through a specific maze. In those settings, if we are able to define a loss
			\(\mathcal{L}_\tau\) for our task \(\tau\) which depends on the parameters
			\(\phi\) of a model, we can express our learning objective as

			\[ \underset{\phi}{\text{min}} \, \mathcal{L}_\tau (\phi) .\]

			We usually find the optimal \(\phi\) by progressively walking along the direction of the gradient of
			\(\mathcal{L}_\tau\) with respect to \(\phi\), i.e.

			\[ \phi \leftarrow \phi - \alpha \nabla_\phi \mathcal{L}_\tau (\phi) ,\]

			also known as gradient descent, where \(\mathcal{L}_\tau\) usually also depends on some data and \(\alpha\)
			is a fixed learning rate,
			controlling the size of the steps we want to take.
		</p>
		<p>Unfortunately, when framing this in a few-shot setting (i.e., with a very small dataset), the above method is
			known to perform poorly on e.g. neural networks, since there is simply too little data for too many parameters. A key insight to MAML is now to
			bypass this problem by learning not only from the data regarding exactly our task, but to learn also from
			data of similar tasks.
			To incorporate this we make an additional assumption, namely that \(\tau\) comes from some distribution of
			tasks \(p(\tau)\) and that we
			can sample freely from this distribution. We can then generalize the above objective to learn how to find an
			optimization strategy for a randomly sampled task from \(p(\tau)\), which we can express as follows:

			\[ \text{min} \, \mathbb{E}_\tau [ \mathcal{L}_\tau (\phi_\tau) ] ,\]

			where \(\tau\) is now a random variable and \(\phi_\tau\) are a set of parameters for task \(\tau\).
			We may use different parameters for each task, use the same parameters for every task or something in
			between. So we have to decide on a parameterization and decide how we actually minimize this new objective.
			In the following we will explore two possible
			answers to these issues.
		</p>
		<h3>Part 1: Just do gradient descent, silly!</h3>
		<p>Of course! The answer to most problems. We can simply learn a single set of parameters \(\phi\) which
			minimizes the expected loss
			over all tasks. Or formally speaking our objective becomes

			\[ \underset{\phi}{\text{min}} \, \mathbb{E}_\tau [ \mathcal{L}_\tau (\phi) ] .\]

			As you can see we chose the most simple parameterization: We learn one parameter vector \(\phi\) that
			minimizes the objective
			across all tasks.
			Further, to actually implement this strategy we can do what is known as <i>Expected Risk Minimization
				(ERM)</i>, which tells us to sample a lot of tasks \(\tau\) and then descent according to

			\[ \phi \leftarrow \phi - \alpha \nabla_\phi \sum_i \mathcal{L}_{\tau_i} (\phi) .\]

			Finn et al. (the authors of MAML) call this type of model <i>pretrained</i>, referring to it simply
			pretraining over all available data and
			then trying to converge on a small amount of samples later.
		</p>
		<h4>Implementing the Pretrained Model</h4>
		<p>
			If the above has gotten all too theoretic for you, take a look at the following <i>gist</i>. It contains a
			simplistic
			implementation of an update step for this <i>pretrained</i> model. It is implemented such as to emphesize
			that
			even if we differentiate between tasks when sampling the batch, the actual optimizer treats each sample the
			same.
		</p>
		<script src="https://gist.github.com/pupuis/cf31417164bc5513302788b906d2a3c2.js"></script>
		<p>
			The implementation is agnostic to the choice of optimizer that you use. To train their <i>pretrained</i>
			model, the MAML creators use
			the <i>Adam</i> optimizer.
		</p>
		<h4>Pretrained Model on a Sinusoid Problem</h4>
		<p>
			In the following figure you can experiment with a <i>pretrained</i> model, which was trained by a collection
			of
			sinusoid regression tasks. The task distribution works as follows: Each task is represented by an amplitude
			\(A\) and a
			phase \(\varphi\) and requires the prediction of sinusoid \(f\):

			\[ f(x) := A \sin(x + \varphi),\]

			where \(A, \varphi\) are sampled uniformly from some predefined range.

			The jist is that different parameters yield different functions \(f_1\) and \(f_2\) with possibly completely
			different function values and gradients.
			Take for example the following two tasks:
			Tasks \(\tau_1, \tau_2\) are both regression tasks on sinusoids \(f_1(x) := \sin (x - \frac{\pi}{2})\) and
			\(f_2(x) := \sin (x + \frac{\pi}{2})\) respectively. These two tasks' function values give completely
			contradicting information, as

			\[ f_1(x) = - f_2(x). \]
		</p>
		<p>
			Before you fit the model, think what
			you would expect to happen based on the position and the
			amount of samples you provided. Feel free to also experiment with the different settings: Distributing the
			samples equispaced or squeezing all of them to a small range
			of the x-axis.
		</p>
		<figure>
			<d-figure id="fitSinePretrained"></d-figure>
		</figure>
		<p>Ouch! That doesn't seem to work that well. Maybe you have already guessed that this would have been to easy.
			The problem that the above approach faces is that the optimal parameters of
			different tasks might live in completely different loss spaces. Given a loss function \(\mathcal{L}\) w.r.t
			some parameters \(\rho\), as well as
			constant parameter-set \(C\) (e.g. a dataset on which the loss is defined), the parameter space defined the
			function values of

			\[ \mathcal{L}(\rho; C) .\]

			You can investigate the problem of different loss spaces for different tasks a little deeper in
			<a href="#twoLossSpaces">this figure</a>. The loss spaces displayed stem from a curve fitting (toy) problem,
			where
			we are interested in parameters \(a, b\)
			and the task requires the prediction of

			\[g(x) = \textbf{elu}(ax + b),\]

			where \(\textbf{elu}\) is the <i>Exponential Linear Unit</i>, which is usually employed as an activation
			function in neural nets, but serves
			us here to make the resulting loss spaces a bit more funky. For reference, this is how the \(\textbf{elu}\)
			is
			defined:

			\[ \textbf{elu} = \begin{cases}
			x & \, x >= 0 \\
			\alpha (\exp(x) - 1) & \, \text{otherwise}.
			\end{cases}
			\]
		</p>
		<figure>
			<d-figure id="twoLossSpaces"></d-figure>
		</figure>
		<p>
			At this point we can conclude that optimizating for parameter-vector \(\phi\) minimizing all task
			distributions at the same time
			is maybe not the best parameter to optimize for. The next approach rather optimizes another variable of the
			problem. Maybe you already have a hunch?
		</p>

		<h3>Part 2: Just do Model-Agnostic Meta-Learning, silly!</h3>
		<p>
			MAML comes to our rescue. We have already seen how different tasks might give us contradicting function
			values at the same query point and how that leads to an undesired ping-pong from one local optimum to
			another, resulting in no meaningful convergence. The idea of MAML is now to
			learn an optimal initialization \(\theta\) in the sense that initializating a task optimizer with these
			parameters and taking gradient descent steps
			from there minimizes the loss for that task. We can express this formally as follows:

			\[ \underset{\theta}{\text{min}} \, \mathbb{E}_\tau [ \mathcal{L}_\tau (U_\tau(\theta)) ] .\]

			where \(U_\tau\) is an optimizer that takes a number of gradient descent steps given the samples of task
			\(\tau\). This is also why MAML is called mode-agnostic, it does
			not make any further assumptions about \(U_\tau\), thus allowing any gradient-based inner optimizer.
		</p>
		<h4>Outline of the Algorithm</h4>
		<p>
			Now that we fixed the objective function, let us briefly take a look at the three main steps of the method,
			given a (current)
			meta-parameter \(\theta\):
		</p>
		<ul>
			<li>1. Sample a number of tasks \(\tau_i\) from \(p(\tau)\).</li>
			<li>2. For each task obtain \(\phi_i = U_{\tau_i}(\theta)\), by minimizing \(\mathcal{L}_{\tau_i,
				\text{train}}(\theta)\)
				on a few training samples.</li>
			<li>3. Update \(\theta\) by gradient descent such that it minimizes \(\mathcal{L}(\theta) := \sum_i
				\mathcal{L}_{\tau_i, \text{test}}(\phi_i)\) on a few
				test samples.</li>
		</ul>
		<p>
			Note that \(\mathcal{L}_{\tau_i, \text{train}}\) and \(\mathcal{L}_{\tau_i, \text{test}}\) are two instances
			of the same loss function
			for different tasks and corresponding training or test data from these tasks.
			Obtaining the \(\phi_i\) is easy, we just do gradient descent:

			\[ \phi_i = \theta - \alpha \nabla_\theta \mathcal{L}_{\tau_i, \text{train}}(\theta).\]

			Further, updating \(\theta\) requires us to evaluate the gradient of the individual task losses on a set of
			test data. We obtain
			the gradient of the overall loss as follows:

			\[ \nabla_\theta \mathcal{L}(\theta) = \sum_{i} \nabla_\theta
			\mathcal{L}_{\tau_i, \text{test}}(\phi_i)
			.\]

			Note that \(\phi_i = U_{\tau_i}(\theta)\) depends on \(\theta\), which means that we have to take a gradient
			through the optimizer
			\(U\). We can then update \(\theta\) via gradient descent, using a new learning rate \(\beta\):

			\[ \theta' = \theta - \beta \nabla_\theta \mathcal{L}(\theta).\]

			And that ü•Å... is more or less everything that comprises the original MAML algorithm.
		</p>
		<h4>Implementing the Algorithm</h4>
		<p>
			However, a machine learning algorithm is not very usefull unless we can execute it on a computer. While
			implementing the <i>pretrained</i>
			model was more or less straightforward, implementing MAML requires some more thinking. Firstly, computing
			\(\phi_i\) is still straightforward, simply call
			the optimization algorithm of your choice (as long as it is gradient-based).
			However, how do we then take the gradient through that optimization algorithm?
			It's actually not that complicated. Almost every modern machine learning framework (e.g. tensorflow), can
			differentiate through
			nearly arbitrary python code. Hence, if we can express our optimizer in a python function, then tensorflow
			can differentiate through it.
		</p>
		<p>
			Below you find a <i>gist</i> that implements a simplistic version of the MAML update step. We coded our
			optimizer ourselves within the function
			<i>fastWeights</i>, but the function also directly applies an input tensor to the optimized weights. We did
			this mainly for simplicity but if you
			are interested in a more thorough reasoning about this design choice, you can read more about it <a
				href="https://gist.github.com/pupuis/f23f483c405b0a169bf279f7b02209bc">in the comments under the
				gist</a>.
		</p>
		<script src="https://gist.github.com/pupuis/f23f483c405b0a169bf279f7b02209bc.js"></script>
		<h4>Will it really work?</h4>
		<p>
			Before we study the MAML model on the sinusoid task distribution, let us spend some
			time on trying to understand MAML intuitively. Why would we expect it to produce better results than the
			pretrained model? To answer this, recall the exercise of <a href="#twoLossSpaces">comparing loss spaces</a>
			from before.
			As stated above, MAML tries to learn an optimal initialization \(\theta\), from which we can easily descent
			both \(\phi_i\) point to the minimum of their respective tasks. This is exactly what MAML does! And now you
			know first hand, why it might actually work.
		</p>
		<h4>Returning to Sinusoids</h4>
		<p>
			After having studied the math behind the MAML objective, as well as its intuition and implementation, it is
			time to evaluate it on the sinusoid example.
			Hopefully, MAML will produce better results
			than the pretrained model (although by now you should be convinced that it will).
			You will have the opportunity to repeat the above experiements on a model that has been trained with
			MAML in <a href="#fitSineMaml">this figure</a>.
			Try to compare the optimization behavior of both the pretrained model and MAML and evaluate for yourself
			whether
			you think the MAML-trained model has found a good meta-initialization parameter \(\theta\).
		</p>
		<figure>
			<d-figure id="fitSineMaml"></d-figure>
		</figure>
		<p>
			So as you were hopefully able to verify, MAML produces results that are way closer to the actual sinusoid,
			despite being exposed to at most 5 samples.
		</p>
		<p>
			The rest of this article is dedicated to introducing interesting extensions of MAML, making the method
			either more robust or more efficient.
			If you are interested in the former, <a href="#section_imaml">jump to iMAML</a>. If you are more interested
			in the latter, the following section sheds
			a light on the computational (in-)efficiency of MAML and what possible solutions to that problem are.
		</p>
		<h2>First-Order Methods: Why differentiating through an optimizer is actually as complicated as it sounds</h2>
		<p>
			In this section we want to gain some understanding of what it actually means to differentiate through \(U\),
			the optimizer.
			Recall the gradient that MAML requires us to compute:

			\[ \nabla_\theta \mathcal{L}(\theta) = \sum_{i} \nabla_\theta
			\mathcal{L}_{\tau_i, \text{test}}(\phi_i)
			.\]

			Expanding
			the summands by applying the chain rule yields

			\[ \nabla_\theta
			\mathcal{L}_{\tau_i, \text{test}}(\phi_i) = \nabla_\theta
			\mathcal{L}_{\tau_i, \text{test}}(U_{\tau_i}(\theta)) = \nabla_{U_{\tau_i}(\theta)} \mathcal{L}_{\tau_i,
			\text{test}}
			\nabla_\theta U_{\tau_i}(\theta) .\]

			Here, \( \nabla_{U_{\tau_i}(\theta)} \mathcal{L}_{\tau_i, \text{test}} \) represents the gradient of the
			loss of task
			\(\tau_i\) by the optimized parameter \(\phi_i\)
			and \(\nabla_\theta U_{\tau_i}(\theta)\) is a gradient through an optimization algorithm. Even if we assume
			that
			the optimizer takes only one gradient descent step, this term becomes

			\[ \nabla_\theta U_{\tau_i}(\theta) = \nabla_\theta (\theta - \alpha \nabla_\theta
			\mathcal{L}_{\tau_i, \text{train}}(\theta) )\]

			which is equal to

			\[ I - \alpha \nabla^2_\theta \mathcal{L}_{\tau_i, \text{train}}(\theta). \]

			Hence, MAML requires us to compute second derivatives in order to optimize \(\theta\), which is
			computationally inefficient, especially
			in high-dimensional problems (such as learning neural nets).
		</p>
		<p>
			We will now spend some time on introducing the two most prominent solutions to this problem, as well as
			comparing them to MAML and to each other.
		</p>




	</dt-article>

	<dt-appendix>
	</dt-appendix>


	<script type="text/bibliography">
@misc{finn2017modelagnostic,
    title={Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks},
    author={Chelsea Finn and Pieter Abbeel and Sergey Levine},
    year={2017},
    eprint={1703.03400},
    archivePrefix={arXiv},
    primaryClass={cs.LG},
		url={https://arxiv.org/abs/1703.03400},
}


@article{lake_one_2011,
	title = {One shot learning of simple visual concepts},
	abstract = {People can learn visual concepts from just one example, but it remains a mystery how this is accomplished. Many authors have proposed that transferred knowledge from more familiar concepts is a route to one shot learning, but what is the form of this abstract knowledge? One hypothesis is that the sharing of parts is core to one shot learning, and we evaluate this idea in the domain of handwritten characters, using a massive new dataset. These simple visual concepts have a rich internal part structure, yet they are particularly tractable for computational models. We introduce a generative model of how characters are composed from strokes, where knowledge from previous characters helps to infer the latent strokes in novel characters. The stroke model outperforms a competing stateof-the-art character model on a challenging one shot learning task, and it provides a good Ô¨Åt to human perceptual data.},
	language = {en},
	author = {Lake, Brenden M and Salakhutdinov, Ruslan and Gross, Jason and Tenenbaum, Joshua B},
	year = {2011},
	url = {https://cims.nyu.edu/~brenden/LakeEtAl2011CogSci.pdf},
}


@article{lake_one-shot_2013,
	title = {One-shot learning by inverting a compositional causal process},
	abstract = {People can learn a new visual class from just one example, yet machine learning algorithms typically require hundreds or thousands of examples to tackle the same problems. Here we present a Hierarchical Bayesian model based on compositionality and causality that can learn a wide range of natural (although simple) visual concepts, generalizing in human-like ways from just one image. We evaluated performance on a challenging one-shot classiÔ¨Åcation task, where our model achieved a human-level error rate while substantially outperforming two deep learning models. We also tested the model on another conceptual task, generating new examples, by using a ‚Äúvisual Turing test‚Äù to show that our model produces human-like performance.},
	language = {en},
	author = {Lake, Brenden M and Salakhutdinov, Ruslan},
	year = {2013},
	url = {https://cims.nyu.edu/~brenden/LakeEtAlNips2013.pdf}
}



@article{koch_siamese_2015,
	title = {Siamese Neural Networks for One-shot Image Recognition},
	abstract = {The process of learning good features for machine learning applications can be very computationally expensive and may prove difÔ¨Åcult in cases where little data is available. A prototypical example of this is the one-shot learning setting, in which we must correctly make predictions given only a single example of each new class. In this paper, we explore a method for learning siamese neural networks which employ a unique structure to naturally rank similarity between inputs. Once a network has been tuned, we can then capitalize on powerful discriminative features to generalize the predictive power of the network not just to new data, but to entirely new classes from unknown distributions. Using a convolutional architecture, we are able to achieve strong results which exceed those of other deep learning models with near state-of-the-art performance on one-shot classiÔ¨Åcation tasks.},
	language = {en},
	author = {Koch, Gregory and Zemel, Richard and Salakhutdinov, Ruslan},
	year = {2015},
	url = {https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf},
}


@article{vinyals_matching_2017,
	title = {Matching Networks for One Shot Learning},
	url = {http://arxiv.org/abs/1606.04080},
	abstract = {Learning from a few examples remains a key challenge in machine learning. Despite recent advances in important domains such as vision and language, the standard supervised deep learning paradigm does not offer a satisfactory solution for learning new concepts rapidly from little data. In this work, we employ ideas from metric learning based on deep neural features and from recent advances that augment neural networks with external memories. Our framework learns a network that maps a small labelled support set and an unlabelled example to its label, obviating the need for fine-tuning to adapt to new class types. We then define one-shot learning problems on vision (using Omniglot, ImageNet) and language tasks. Our algorithm improves one-shot accuracy on ImageNet from 87.6\% to 93.2\% and from 88.0\% to 93.8\% on Omniglot compared to competing approaches. We also demonstrate the usefulness of the same model on language modeling by introducing a one-shot task on the Penn Treebank.},
	urldate = {2021-06-12},
	author = {Vinyals, Oriol and Blundell, Charles and Lillicrap, Timothy and Kavukcuoglu, Koray and Wierstra, Daan},
	month = dec,
	year = {2017},
	note = {arXiv: 1606.04080},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning}
}


@article{santoro_meta-learning_2016,
	title = {Meta-Learning with Memory-Augmented Neural Networks},
	abstract = {Despite recent breakthroughs in the applications of deep neural networks, one setting that presents a persistent challenge is that of ‚Äúone-shot learning.‚Äù Traditional gradient-based networks require a lot of data to learn, often through extensive iterative training. When new data is encountered, the models must inefÔ¨Åciently relearn their parameters to adequately incorporate the new information without catastrophic interference. Architectures with augmented memory capacities, such as Neural Turing Machines (NTMs), offer the ability to quickly encode and retrieve new information, and hence can potentially obviate the downsides of conventional models. Here, we demonstrate the ability of a memory-augmented neural network to rapidly assimilate new data, and leverage this data to make accurate predictions after only a few samples. We also introduce a new method for accessing an external memory that focuses on memory content, unlike previous methods that additionally use memory locationbased focusing mechanisms.},
	language = {en},
	author = {Santoro, Adam and Bartunov, Sergey and Botvinick, Matthew and Wierstra, Daan and Lillicrap, Timothy},
	year = {2016},
	url = {https://web.stanford.edu/class/psych209/Readings/Santoro16MetaLearningWithMemAugNNs.pdf},
}



@article{andrychowicz_learning_2016,
	title = {Learning to learn by gradient descent by gradient descent},
	url = {http://arxiv.org/abs/1606.04474},
	abstract = {The move from hand-designed features to learned features in machine learning has been wildly successful. In spite of this, optimization algorithms are still designed by hand. In this paper we show how the design of an optimization algorithm can be cast as a learning problem, allowing the algorithm to learn to exploit structure in the problems of interest in an automatic way. Our learned algorithms, implemented by LSTMs, outperform generic, hand-designed competitors on the tasks for which they are trained, and also generalize well to new tasks with similar structure. We demonstrate this on a number of tasks, including simple convex problems, training neural networks, and styling images with neural art.},
	urldate = {2021-06-12},
	author = {Andrychowicz, Marcin and Denil, Misha and Gomez, Sergio and Hoffman, Matthew W. and Pfau, David and Schaul, Tom and Shillingford, Brendan and de Freitas, Nando},
	month = nov,
	year = {2016},
	note = {arXiv: 1606.04474},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}



@article{snell_prototypical_2017,
	title = {Prototypical Networks for Few-shot Learning},
	url = {http://arxiv.org/abs/1703.05175},
	abstract = {We propose prototypical networks for the problem of few-shot classiÔ¨Åcation, where a classiÔ¨Åer must generalize to new classes not seen in the training set, given only a small number of examples of each new class. Prototypical networks learn a metric space in which classiÔ¨Åcation can be performed by computing distances to prototype representations of each class. Compared to recent approaches for few-shot learning, they reÔ¨Çect a simpler inductive bias that is beneÔ¨Åcial in this limited-data regime, and achieve excellent results. We provide an analysis showing that some simple design decisions can yield substantial improvements over recent approaches involving complicated architectural choices and meta-learning. We further extend prototypical networks to zero-shot learning and achieve state-of-theart results on the CU-Birds dataset.},
	language = {en},
	urldate = {2021-06-11},
	author = {Snell, Jake and Swersky, Kevin and Zemel, Richard S.},
	month = jun,
	year = {2017},
	note = {arXiv: 1703.05175},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}



@article{munkhdalai_meta_2017,
	title = {Meta Networks},
	url = {http://arxiv.org/abs/1703.00837},
	abstract = {Neural networks have been successfully applied in applications with a large amount of labeled data. However, the task of rapid generalization on new concepts with small training data while preserving performances on previously learned ones still presents a significant challenge to neural network models. In this work, we introduce a novel meta learning method, Meta Networks (MetaNet), that learns a meta-level knowledge across tasks and shifts its inductive biases via fast parameterization for rapid generalization. When evaluated on Omniglot and Mini-ImageNet benchmarks, our MetaNet models achieve a near human-level performance and outperform the baseline approaches by up to 6\% accuracy. We demonstrate several appealing properties of MetaNet relating to generalization and continual learning.},
	urldate = {2021-06-12},
	author = {Munkhdalai, Tsendsuren and Yu, Hong},
	month = jun,
	year = {2017},
	note = {arXiv: 1703.00837},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Accepted at ICML 2017 - rewrote: the main section; added: MetaNet algorithmic procedure; performed: Mini-ImageNet evaluation},
}




@article{andrychowicz_learning_2016,
	title = {Learning to learn by gradient descent by gradient descent},
	url = {http://arxiv.org/abs/1606.04474},
	abstract = {The move from hand-designed features to learned features in machine learning has been wildly successful. In spite of this, optimization algorithms are still designed by hand. In this paper we show how the design of an optimization algorithm can be cast as a learning problem, allowing the algorithm to learn to exploit structure in the problems of interest in an automatic way. Our learned algorithms, implemented by LSTMs, outperform generic, hand-designed competitors on the tasks for which they are trained, and also generalize well to new tasks with similar structure. We demonstrate this on a number of tasks, including simple convex problems, training neural networks, and styling images with neural art.},
	urldate = {2021-06-12},
	journal = {arXiv:1606.04474 [cs]},
	author = {Andrychowicz, Marcin and Denil, Misha and Gomez, Sergio and Hoffman, Matthew W. and Pfau, David and Schaul, Tom and Shillingford, Brendan and de Freitas, Nando},
	month = nov,
	year = {2016},
	note = {arXiv: 1606.04474},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing}
}


@article{rajeswaran_meta-learning_2019,
	title = {Meta-Learning with Implicit Gradients},
	url = {http://arxiv.org/abs/1909.04630},
	abstract = {A core capability of intelligent systems is the ability to quickly learn new tasks by drawing on prior experience. Gradient (or optimization) based meta-learning has recently emerged as an effective approach for few-shot learning. In this formulation, meta-parameters are learned in the outer loop, while task-specific models are learned in the inner-loop, by using only a small amount of data from the current task. A key challenge in scaling these approaches is the need to differentiate through the inner loop learning process, which can impose considerable computational and memory burdens. By drawing upon implicit differentiation, we develop the implicit MAML algorithm, which depends only on the solution to the inner level optimization and not the path taken by the inner loop optimizer. This effectively decouples the meta-gradient computation from the choice of inner loop optimizer. As a result, our approach is agnostic to the choice of inner loop optimizer and can gracefully handle many gradient steps without vanishing gradients or memory constraints. Theoretically, we prove that implicit MAML can compute accurate meta-gradients with a memory footprint that is, up to small constant factors, no more than that which is required to compute a single inner loop gradient and at no overall increase in the total computational cost. Experimentally, we show that these benefits of implicit MAML translate into empirical gains on few-shot image recognition benchmarks.},
	urldate = {2021-04-30},
	journal = {arXiv:1909.04630 [cs, math, stat]},
	author = {Rajeswaran, Aravind and Finn, Chelsea and Kakade, Sham and Levine, Sergey},
	month = sep,
	year = {2019},
	note = {arXiv: 1909.04630},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Optimization and Control},
}

@article{grant_recasting_2018,
	title = {Recasting Gradient-Based Meta-Learning as Hierarchical Bayes},
	url = {http://arxiv.org/abs/1801.08930},
	abstract = {Meta-learning allows an intelligent agent to leverage prior learning episodes as a basis for quickly improving performance on a novel task. Bayesian hierarchical modeling provides a theoretical framework for formalizing meta-learning as inference for a set of parameters that are shared across tasks. Here, we reformulate the model-agnostic meta-learning algorithm (MAML) of Finn et al. (2017) as a method for probabilistic inference in a hierarchical Bayesian model. In contrast to prior methods for meta-learning via hierarchical Bayes, MAML is naturally applicable to complex function approximators through its use of a scalable gradient descent procedure for posterior inference. Furthermore, the identification of MAML as hierarchical Bayes provides a way to understand the algorithm's operation as a meta-learning procedure, as well as an opportunity to make use of computational strategies for efficient inference. We use this opportunity to propose an improvement to the MAML algorithm that makes use of techniques from approximate inference and curvature estimation.},
	urldate = {2021-06-30},
	journal = {arXiv:1801.08930 [cs]},
	author = {Grant, Erin and Finn, Chelsea and Levine, Sergey and Darrell, Trevor and Griffiths, Thomas},
	month = jan,
	year = {2018},
	note = {arXiv: 1801.08930},
	keywords = {Computer Science - Machine Learning},
}


@article{blondel_efficient_2021,
	title = {Efficient and Modular Implicit Differentiation},
	url = {http://arxiv.org/abs/2105.15183},
	abstract = {Automatic differentiation (autodiff) has revolutionized machine learning. It allows expressing complex computations by composing elementary ones in creative ways and removes the burden of computing their derivatives by hand. More recently, differentiation of optimization problem solutions has attracted widespread attention with applications such as optimization as a layer, and in bi-level problems such as hyper-parameter optimization and meta-learning. However, the formulas for these derivatives often involve case-by-case tedious mathematical derivations. In this paper, we propose a uniÔ¨Åed, efÔ¨Åcient and modular approach for implicit differentiation of optimization problems. In our approach, the user deÔ¨Ånes (in Python in the case of our implementation) a function F capturing the optimality conditions of the problem to be differentiated. Once this is done, we leverage autodiff of F and implicit differentiation to automatically differentiate the optimization problem. Our approach thus combines the beneÔ¨Åts of implicit differentiation and autodiff. It is efÔ¨Åcient as it can be added on top of any state-of-the-art solver and modular as the optimality condition speciÔ¨Åcation is decoupled from the implicit differentiation mechanism. We show that seemingly simple principles allow to recover many recently proposed implicit differentiation methods and create new ones easily. We demonstrate the ease of formulating and solving bi-level optimization problems using our framework. We also showcase an application to the sensitivity analysis of molecular dynamics.},
	language = {en},
	urldate = {2021-07-03},
	journal = {arXiv:2105.15183 [cs, math, stat]},
	author = {Blondel, Mathieu and Berthet, Quentin and Cuturi, Marco and Frostig, Roy and Hoyer, Stephan and Llinares-L√≥pez, Felipe and Pedregosa, Fabian and Vert, Jean-Philippe},
	month = may,
	year = {2021},
	note = {arXiv: 2105.15183},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Numerical Analysis},
}

@article{finn_probabilistic_2019,
	title = {Probabilistic Model-Agnostic Meta-Learning},
	url = {http://arxiv.org/abs/1806.02817},
	abstract = {Meta-learning for few-shot learning entails acquiring a prior over previous tasks and experiences, such that new tasks be learned from small amounts of data. However, a critical challenge in few-shot learning is task ambiguity: even when a powerful prior can be meta-learned from a large number of prior tasks, a small dataset for a new task can simply be too ambiguous to acquire a single model (e.g., a classifier) for that task that is accurate. In this paper, we propose a probabilistic meta-learning algorithm that can sample models for a new task from a model distribution. Our approach extends model-agnostic meta-learning, which adapts to new tasks via gradient descent, to incorporate a parameter distribution that is trained via a variational lower bound. At meta-test time, our algorithm adapts via a simple procedure that injects noise into gradient descent, and at meta-training time, the model is trained such that this stochastic adaptation procedure produces samples from the approximate model posterior. Our experimental results show that our method can sample plausible classifiers and regressors in ambiguous few-shot learning problems. We also show how reasoning about ambiguity can also be used for downstream active learning problems.},
	urldate = {2021-04-30},
	journal = {arXiv:1806.02817 [cs, stat]},
	author = {Finn, Chelsea and Xu, Kelvin and Levine, Sergey},
	month = oct,
	year = {2019},
	note = {arXiv: 1806.02817
version: 2},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: NeurIPS 2018. First two authors contributed equally. Supplementary results available at https://sites.google.com/view/probabilistic-maml/},
}



@misc{lake_omniglot_2015,
	title = {Omniglot data set for one-shot learning},
	url = {https://github.com/brendenlake/omniglot},
	abstract = {The Omniglot data set is designed for developing more human-like learning algorithms. It contains 1623 different handwritten characters from 50 different alphabets. Each of the 1623 characters was drawn online via Amazon's Mechanical Turk by 20 different people. Each image is paired with stroke data, a sequences of [x,y,t] coordinates with time (t) in milliseconds.},
	urldate = {2021-07-07},
	journal = {Omniglot data set for one-shot learning},
	author = {Lake, Brenden M and Salakhutdinov, Ruslan and Tenenbaum, Joshua B},
	month = oct,
	year = {2015},
}

@misc{liu_tools_2019,
	title = {Tools for mini-ImageNet Dataset},
	journal = {Tools for mini-ImageNet Dataset},
	author = {Liu, Yaoyao},
	month = jan,
	year = {2019},
	url = {https://github.com/yaoyao-liu/mini-imagenet-tools}
}


@article{glorot_deep_nodate,
	title = {Deep {Sparse} {RectiÔ¨Åer} {Neural} {Networks}},
	abstract = {While logistic sigmoid neurons are more biologically plausible than hyperbolic tangent neurons, the latter work better for training multi-layer neural networks. This paper shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hyperbolic tangent networks in spite of the hard non-linearity and non-diÔ¨Äerentiability at zero, creating sparse representations with true zeros, which seem remarkably suitable for naturally sparse data. Even though they can take advantage of semi-supervised setups with extra-unlabeled data, deep rectiÔ¨Åer networks can reach their best performance without requiring any unsupervised pre-training on purely supervised tasks with large labeled datasets. Hence, these results can be seen as a new milestone in the attempts at understanding the diÔ¨Éculty in training deep but purely supervised neural networks, and closing the performance gap between neural networks learnt with and without unsupervised pre-training.},
	language = {en},
	author = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
	pages = {9},
	file = {Glorot et al. - Deep Sparse RectiÔ¨Åer Neural Networks.pdf:/home/luis/Zotero/storage/IF9UR5S4/Glorot et al. - Deep Sparse RectiÔ¨Åer Neural Networks.pdf:application/pdf},
}

@article{sagun_eigenvalues_2017,
	title = {Eigenvalues of the {Hessian} in {Deep} {Learning}: {Singularity} and {Beyond}},
	shorttitle = {Eigenvalues of the {Hessian} in {Deep} {Learning}},
	url = {http://arxiv.org/abs/1611.07476},
	abstract = {We look at the eigenvalues of the Hessian of a loss function before and after training. The eigenvalue distribution is seen to be composed of two parts, the bulk which is concentrated around zero, and the edges which are scattered away from zero. We present empirical evidence for the bulk indicating how overparametrized the system is, and for the edges that depend on the input data.},
	language = {en},
	urldate = {2021-07-12},
	journal = {arXiv:1611.07476 [cs]},
	author = {Sagun, Levent and Bottou, Leon and LeCun, Yann},
	month = oct,
	year = {2017},
	note = {arXiv: 1611.07476},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: ICLR submission, 2016 - updated to match the openreview.net version},
	file = {Sagun et al. - 2017 - Eigenvalues of the Hessian in Deep Learning Singu.pdf:/home/luis/Zotero/storage/MZ5AFFIZ/Sagun et al. - 2017 - Eigenvalues of the Hessian in Deep Learning Singu.pdf:application/pdf},
}

@article{park_meta-curvature_2020,
	title = {Meta-{Curvature}},
	url = {http://arxiv.org/abs/1902.03356},
	abstract = {We propose meta-curvature (MC), a framework to learn curvature information for better generalization and fast model adaptation. MC expands on the modelagnostic meta-learner (MAML) by learning to transform the gradients in the inner optimization such that the transformed gradients achieve better generalization performance to a new task. For training large scale neural networks, we decompose the curvature matrix into smaller matrices in a novel scheme where we capture the dependencies of the model‚Äôs parameters with a series of tensor products. We demonstrate the effects of our proposed method on several few-shot learning tasks and datasets. Without any task speciÔ¨Åc techniques and architectures, the proposed method achieves substantial improvement upon previous MAML variants and outperforms the recent state-of-the-art methods. Furthermore, we observe faster convergence rates of the meta-training process. Finally, we present an analysis that explains better generalization performance with the meta-trained curvature.},
	language = {en},
	urldate = {2021-07-12},
	journal = {arXiv:1902.03356 [cs, stat]},
	author = {Park, Eunbyung and Oliva, Junier B.},
	month = jan,
	year = {2020},
	note = {arXiv: 1902.03356},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: To appear in NeurIPS 2019},
	file = {Park and Oliva - 2020 - Meta-Curvature.pdf:/home/luis/Zotero/storage/ZGIXZNK4/Park and Oliva - 2020 - Meta-Curvature.pdf:application/pdf},
}

@article{nichol_first-order_2018,
	title = {On {First}-{Order} {Meta}-{Learning} {Algorithms}},
	url = {http://arxiv.org/abs/1803.02999},
	abstract = {This paper considers meta-learning problems, where there is a distribution of tasks, and we would like to obtain an agent that performs well (i.e., learns quickly) when presented with a previously unseen task sampled from this distribution. We analyze a family of algorithms for learning a parameter initialization that can be Ô¨Åne-tuned quickly on a new task, using only Ô¨Årstorder derivatives for the meta-learning updates. This family includes and generalizes Ô¨Årst-order MAML, an approximation to MAML obtained by ignoring second-order derivatives. It also includes Reptile, a new algorithm that we introduce here, which works by repeatedly sampling a task, training on it, and moving the initialization towards the trained weights on that task. We expand on the results from Finn et al. showing that Ô¨Årst-order meta-learning algorithms perform well on some well-established benchmarks for few-shot classiÔ¨Åcation, and we provide theoretical analysis aimed at understanding why these algorithms work.},
	language = {en},
	urldate = {2021-07-12},
	journal = {arXiv:1803.02999 [cs]},
	author = {Nichol, Alex and Achiam, Joshua and Schulman, John},
	month = oct,
	year = {2018},
	note = {arXiv: 1803.02999},
	keywords = {Computer Science - Machine Learning},
	file = {Nichol et al. - 2018 - On First-Order Meta-Learning Algorithms.pdf:/home/luis/Zotero/storage/L8HLWK9P/Nichol et al. - 2018 - On First-Order Meta-Learning Algorithms.pdf:application/pdf},
}

</script>
</body>

</html>
